{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Moseli Motsoehli\n",
    "\n",
    "### Seq to Seq model for comparing text similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__I use the quora question pairs dataset as a test for this although the model is eventually used for text transcribed from recorded speech as a way to evaluate  public speaking__\n",
    "\n",
    "I reuse mose of my code from a combination of a text summarization project with Jan Platos and preprocessing from a topic modelling project for ICS661. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Deeps\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#### Libraries\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "import logging\n",
    "from collections import Counter\n",
    "from  tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "from Generator import *\n",
    "\n",
    "#import gensim as gs\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sb\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as k\n",
    "#k.set_learning_phase(1)\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Dense,LSTM,Input,Activation,Add,TimeDistributed,\\\n",
    "Permute,Flatten,RepeatVector,merge,Lambda,Multiply,Reshape,Embedding,Bidirectional,Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#bert\n",
    "from bert_embedding import BertEmbedding as Bert\n",
    "\n",
    "#model Freezing\n",
    "import tensorflow as tf\n",
    "# freeze_graph \"screenshots\" the graph\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "# optimize_for_inference lib optimizes this frozen graph\n",
    "from tensorflow.python.tools import optimize_for_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_SIZE=15\n",
    "BATCH_SIZE=32\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 1000\n",
      "Number of examples after removing NA: 1000\n",
      "\n",
      "\n",
      "   id  qid1  qid2                                          question1  \\\n",
      "0   0     1     2  What is the step by step guide to invest in sh...   \n",
      "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0  What is the step by step guide to invest in sh...             0  \n",
      "1  What would happen if the Indian government sto...             0  \n",
      "\n",
      "\n",
      "      id  qid1  qid2                                          question1  \\\n",
      "998  998  1991  1992  Could we use cherenkov atmosphere radiation (w...   \n",
      "999  999  1993  1994               What is a good song for lyric prank?   \n",
      "\n",
      "                                             question2  is_duplicate  \n",
      "998  Can we map the surface (and the subsurface) of...             1  \n",
      "999                     Diving the Blue Hole in Dahab?             0  \n",
      "\n",
      "\n",
      "propotion of positives: 0.38\n"
     ]
    }
   ],
   "source": [
    "datafile = \"train.csv\"\n",
    "\n",
    "data =  pd.read_csv(datafile,sep=\",\")[:][:1000]\n",
    "print(\"Number of examples: %s\"%len(data))\n",
    "data=data.dropna(subset=['question1','question2', 'is_duplicate'])\n",
    "data=data.reset_index(drop=True)\n",
    "print(\"Number of examples after removing NA: %s\"%len(data))\n",
    "print(\"\\n\")\n",
    "print(data.head(2))\n",
    "print(\"\\n\")\n",
    "print(data.tail(2))\n",
    "print(\"\\n\")\n",
    "print(\"propotion of positives: %s\"%np.mean(data[\"is_duplicate\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing addopted from my Topic Model in ICS661 AKA Thematron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special(text):\n",
    "    special_chars = \"[~#$%^@&*&()+-_\\\",?!.:[]\\\\;><`|{}=\\/\\'=»¿シし]\"\n",
    "    for k in special_chars:\n",
    "            if type(text) == str:\n",
    "                if k==\"-\" or k==\"_\":\n",
    "                    text=text.replace(k, \"\")\n",
    "                else:\n",
    "                    text=text.replace(k, \" \")       \n",
    "    return text\n",
    "\n",
    "def contractions(sent):\n",
    "    sub_pattern = [(\"will not\",\"won't\"),(\"shall not\",\"shan't\"),\n",
    "                (\" not\", \"n\\'t\"),(\" will\",\"\\'ll\"),(\" is\",\"\\'s\"),\n",
    "                   (\" am\",\"\\'m\"),(\" are\",\"\\'re\"),(\" is\",\"who\\'s\")]\n",
    "    sent2=sent.split(\" \")\n",
    "    hold = \"\"\n",
    "    for k in range(len(sent2)):\n",
    "        kk = sent2[k]\n",
    "        for rep in range(len(sub_pattern)):\n",
    "            kk = re.sub(sub_pattern[rep][1],sub_pattern[rep][0],kk)\n",
    "        hold = hold + \" \" + kk\n",
    "    return hold.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stops =  set(stopwords.words('english'))\n",
    "    stops1 = [word.lower() for word in stops]\n",
    "    punctuation = [',','.','!','?',';','-']\n",
    "    hold = []\n",
    "    if type(text) == list:\n",
    "        for word in range(len(text)):\n",
    "            if text[word].lower() in stops1 or text[word].lower() in punctuation or text[word].lower() == \"xxxxxx\":\n",
    "                continue\n",
    "            else:\n",
    "                hold.append(text[word].lower())\n",
    "    return hold\n",
    "\n",
    "def wordTok(sent):\n",
    "    tok = wt(sent)\n",
    "    return tok\n",
    "\n",
    "def pipeline(text):\n",
    "    text = contractions(text)\n",
    "    text = remove_special(text)\n",
    "    textToks = wordTok(text)\n",
    "    #textToks = remove_stop_words(textToks)\n",
    "    final = \"\"\n",
    "    for k in range(len(textToks)):\n",
    "            final = final+textToks[k]+\" \"\n",
    "    return final.strip().lower()\n",
    "\n",
    "########################################################\n",
    "########create corpus and create word vectors and training data###########\n",
    "def createCorpus(t):\n",
    "    corpus = []\n",
    "    all_sent = []\n",
    "    for k in t:\n",
    "        for p in t[k]:\n",
    "            corpus.append(st(p))\n",
    "    for sent in range(len(corpus)):\n",
    "        for k in corpus[sent]:\n",
    "            all_sent.append(k)\n",
    "    for m in range(len(all_sent)):\n",
    "        all_sent[m] = wt(all_sent[m])\n",
    "    \n",
    "    all_words=[]\n",
    "    for sent in all_sent:\n",
    "        hold=[]\n",
    "        for word in sent:\n",
    "            hold.append(word.lower())\n",
    "        all_words.append(hold)\n",
    "    return all_words\n",
    "\n",
    "def wordvecmatrix(model1,data):\n",
    "    IO_data={\"question1\":[],\"question2\":[],\"label\":[]}\n",
    "    pbar = tqdm(range(len(data[\"question1\"])))\n",
    "    for k in range(len(data[\"question1\"])):\n",
    "        q1=[]\n",
    "        q2=[]\n",
    "        label=[]\n",
    "        for word in data[\"question1\"][k]:\n",
    "            try:\n",
    "                q1.append(model1.wv.word_vec(word))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        for word in data[\"question2\"][k]:\n",
    "            try:\n",
    "                q2.append(model1.wv.word_vec(word))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        IO_data[\"label\"].append(data[\"is_duplicate\"][k])\n",
    "        IO_data[\"question1\"].append(q1) \n",
    "        IO_data[\"question2\"].append(q2)\n",
    "        pbar.update(1)\n",
    "    print('\\007')\n",
    "    pbar.close()\n",
    "    return IO_data\n",
    "\n",
    "def sequence_padding(stringlist):\n",
    "    newstring = pad_sequences(stringlist, maxlen=STRING_SIZE,\n",
    "                              dtype=object,padding='post',\n",
    "                              truncating='post', value=\"PAD\")\n",
    "    #newstring[-1] =\"#\"\n",
    "    return newstring\n",
    "\n",
    "def list2string(listinput):\n",
    "    hold=\"\"\n",
    "    for k in listinput:\n",
    "        hold = hold +k.strip()+\" \"\n",
    "    return hold.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show two comparison sentences\n",
      "--------------------------------------------\n",
      "1:  what is the story of kohinoor kohinoor diamond\n",
      "--------------------------------------------\n",
      "2:  what would happen if the indian government stole the kohinoor kohinoor diamond back\n",
      "--------------------------------------------\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data[\"question1\"] = list(map(pipeline,data[\"question1\"]))\n",
    "data[\"question2\"] = list(map(pipeline,data[\"question2\"]))\n",
    "\n",
    "\n",
    "print(\"Show two comparison sentences\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"1: \",data[\"question1\"][1])\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"2: \",data[\"question2\"][1])\n",
    "print(\"--------------------------------------------\")\n",
    "print(data[\"is_duplicate\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus and Wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 5864.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'can', 'i', 'increase', 'the', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'a', 'vpn', 'PAD']\n",
      "15\n",
      "['how', 'can', 'internet', 'speed', 'be', 'increased', 'by', 'hacking', 'through', 'dns', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "sentences1 = list(map(wordTok,data[\"question1\"]))\n",
    "sentences2 = list(map(wordTok,data[\"question2\"]))\n",
    "\n",
    "####Truncade and Pad to 20 words max\n",
    "sentences1 = list(map(list,sequence_padding(sentences1)))\n",
    "sentences2 = list(map(list,sequence_padding(sentences2)))\n",
    "\n",
    "data[\"question1\"] = sentences1\n",
    "data[\"question2\"] = sentences2\n",
    "dataAll = [data[\"question1\"],data[\"question2\"]] \n",
    "dataAll = pd.concat(dataAll)\n",
    "\n",
    "model1 = Word2Vec(size = 300,sg=1,compute_loss=True,window=4,\n",
    "                 min_count=1,workers=4)\n",
    "model1.build_vocab(dataAll)  # prepare the model vocabulary\n",
    "model1.train(dataAll, total_examples=model1.corpus_count, epochs=100)\n",
    "print(model1.get_latest_training_loss())\n",
    "\n",
    "#model2 = Word2Vec(size = 150,sg=1,compute_loss=True,window=4,\n",
    "#                 min_count=1,workers=4)\n",
    "#model2.build_vocab(sentences2)  # prepare the model vocabulary\n",
    "#model2.train(sentences2, total_examples=model2.corpus_count, epochs=100)\n",
    "#print(model2.get_latest_training_loss())\n",
    "\n",
    "final_data = wordvecmatrix(model1,data)\n",
    "print(sentences1[2])\n",
    "print(len(final_data[\"question1\"][2]))\n",
    "print(sentences2[2])\n",
    "print(len(final_data[\"question2\"][2]))\n",
    "del sentences1\n",
    "del sentences2\n",
    "del dataAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8735039085149765"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(final_data).head(10)\n",
    "#print(model1.wv.word_vec(\"internet\"))\n",
    "#print(model2.wv.word_vec(\"internet\"))\n",
    "cosine(model1.wv.word_vec(\"speed\"),model1.wv.word_vec(\"men\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAX4UlEQVR4nO3df5RV5X3v8fdHQCklJkbGHzjKoEmJlOAwOVAjESk1QmhKQuJKZJGYRAjmer3XpgEXKXcprq5ktf4oiaGLBoFSrUVbJGk05qppwsUkxDL8xuAPYokZJDJgYvyBCeD3/nH2wDieM+fMPoMDz3xea501ez/72ft897OWHzf77PMcRQRmZpauE3q6ADMzO7oc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmietbqYOkZcCHgT0RMSJrmw98HmjNuv11RDxYYt8vAjOBALYCn4uI1yq956BBg6KhoaHKUzAzs/Xr1++NiLpS2yoGPbAcWAjc2aF9QUTcWm4nSWcB/xsYHhH7Jf0bcEV2vE41NDTQ3NxcRWlmZgYg6RfltlW8dRMRa4AXcr53X+APJPUFBgDP5TyOmZnlVMs9+mslbZG0TNIpHTdGxC7gVuBZYDfwYkQ8XMP7mZlZDnmDfhFwHtBIMcRv69ghC/+PAEOBwcAfSvpUuQNKmiWpWVJza2truW5mZtZF1dyjf5OIeL5tWdIdwAMlul0K/HdEtGb9VgEXAf9S5piLgcUAhULBE/CYJeLAgQO0tLTw2msVn8OwKvTv35/6+nr69etX9T65gl7SmRGxO1udCmwr0e1Z4EJJA4D9wJ8B/oTVrJdpaWnhbW97Gw0NDUjq6XKOaxHBvn37aGlpYejQoVXvV/HWjaQVwFpgmKQWSTOAmyVtlbQF+FPgi1nfwZIezAp6DFgJbKD4aOUJZFfsZtZ7vPbaa5x66qkO+W4giVNPPbXL/zqqeEUfEdNKNC8t0/c5YHK79RuBG7tUkZklxyHfffKMpb8Za2aWOAe9mVkNvvrVr75h/aKLLsp9rIULF/Kud70LSezdu7fW0g5z0JuZ1aBj0P/kJz/JfayxY8fy/e9/nyFDhtRa1hs46M0seV/5ylcYNmwYl156KdOmTePWW29l/Pjxh6da2bt3L23zax06dIg5c+YwevRoRo4cyTe/+U0Adu/ezbhx42hsbGTEiBE8+uijzJ07l/3799PY2Mj06dMBGDhwIFB8QmbOnDmMGDGC9773vdx7770ArF69mvHjx3P55Zfznve8h+nTp9P2k66jRo3iaMzzlevxSjOzPG66/3F+9txvu/WYwwefzI1/8cdlt69fv5577rmHjRs3cvDgQZqamnjf+95Xtv/SpUt5+9vfzrp16/jd737H2LFjueyyy1i1ahUTJ05k3rx5HDp0iFdffZWLL76YhQsXsmnTpjcdZ9WqVWzatInNmzezd+9eRo8ezbhx4wDYuHEjjz/+OIMHD2bs2LH8+Mc/5gMf+EDtg1GGg97Mkvboo48ydepUBgwYAMCUKVM67f/www+zZcsWVq5cCcCLL77I008/zejRo7nqqqs4cOAAH/3oR2lsbOz0OD/60Y+YNm0affr04fTTT+eSSy5h3bp1nHzyyYwZM4b6+noAGhsb2blzp4PezNLQ2ZX30VTqkcS+ffvy+uuvA7zhufSI4Bvf+AYTJ0580z5r1qzhu9/9Lp/+9KeZM2cOV155Zdn3bLsdU8pJJ510eLlPnz4cPHiwqvPIy/fozSxp48aN41vf+hb79+/npZde4v777weK06GvX78e4PDVO8DEiRNZtGgRBw4cAOCpp57ilVde4Re/+AWnnXYan//855kxYwYbNmwAoF+/fof7dnzfe++9l0OHDtHa2sqaNWsYM2bM0T7dkhz0Zpa0pqYmPvnJT9LY2MjHP/5xLr74YgBmz57NokWLuOiii97wKOPMmTMZPnw4TU1NjBgxgquvvpqDBw+yevVqGhsbGTVqFPfddx/XXXcdALNmzWLkyJGHP4xtM3XqVEaOHMkFF1zAhAkTuPnmmznjjDM6rfX222+nvr6elpYWRo4cycyZM7tlDNTZPy96SqFQCP/wiFkatm/fzvnnn9/TZRw2f/58Bg4cyOzZs3u6lNxKjamk9RFRKNXfV/RmZonzh7Fm1qvMnz+/p0t4y/mK3swscQ56M7PEOejNzBLnoDczS5yD3sysBt05TfH06dMZNmwYI0aMODzdQndw0JuZ1aA7pymePn06TzzxBFu3bmX//v0sWbKk1vKA6n4zdpmkPZK2tWubL2mXpE3Za3KZfd8haaWkJyRtl/T+bqnazKwLjpdpiidPnowkJDFmzBhaWlq65fyreY5+ObAQuLND+4KIuLXCvl8H/m9EXC7pRGBA10s0s2R8by78amv3HvOM98KH/rbs5uNxmuIDBw5w11138fWvf72GgTmimh8HXyOpoasHlnQyMA74bHac3wO/7+pxzMxqcTxOU3zNNdcwbty4w/Py1KqWb8ZeK+lKoBn4UkT8usP2c4FW4J8kXQCsB66LiFdKHUzSLGAWwDnnnFNDWWZ2zOrkyvtoOp6mKb7ppptobW09fMuoO+T9MHYRcB7QCOwGbivRpy/QBCyKiFHAK8DccgeMiMURUYiIQl1dXc6yzMze6HiapnjJkiU89NBDrFixghNO6L5nZXJd0UfE823Lku4AHijRrQVoiYjHsvWVdBL0ZmZHQ/tpiocMGfKGaYo/8YlPcNdddzFhwoTD/WfOnMnOnTtpamoiIqirq+Pb3/42q1ev5pZbbqFfv34MHDiQO+8sfmzZNk1xU1MTd9999+HjTJ06lbVr13LBBRcg6fA0xU888UTZWr/whS8wZMgQ3v/+4nMrH/vYx7jhhhtqHoOqpinO7tE/EBEjsvUzI2J3tvxF4E8i4ooS+z0KzIyIJyXNB/4wIuZUej9PU2yWDk9T3P26Ok1xxSt6SSuA8cAgSS3AjcB4SY1AADuBq7O+g4ElEdH2uOX/Au7Onrh5BvhcjnMyM7MaVPPUzbQSzUvL9H0OmNxufRNQ8v8wZmY9wdMUm5kdBcfiL9kdr/KMpYPezI6q/v37s2/fPod9N4gI9u3bR//+/bu0n39hysyOqrYfu25tbe3pUpLQv3//w1+2qpaD3syOqn79+jF06NCeLqNX860bM7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEVQx6Scsk7ZG0rV3bfEm7JG3KXpM72b+PpI2SSv2AuJmZHWXVXNEvByaVaF8QEY3Z68FO9r8O2J6nODMzq13FoI+INcALeQ4uqR74c2BJnv3NzKx2tdyjv1bSluzWzill+nwNuB54vdLBJM2S1Cyp2b9EY2bWffIG/SLgPKAR2A3c1rGDpA8DeyJifTUHjIjFEVGIiEJdXV3OsszMrKNcQR8Rz0fEoYh4HbgDGFOi21hgiqSdwD3ABEn/krtSMzPLJVfQSzqz3epUYFvHPhHx5Yioj4gG4ArgBxHxqVxVmplZbhV/HFzSCmA8MEhSC3AjMF5SIxDATuDqrO9gYElElH3c0szM3lqKiJ6u4U0KhUI0Nzf3dBlmZscNSesjolBqm78Za2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWuIpBL2mZpD2StrVrmy9pl6RN2etNPx0o6WxJP5S0XdLjkq7r7uLNzKyyaq7olwOTSrQviIjG7PVgie0HgS9FxPnAhcD/lDQ8f6lmZpZHxaCPiDXAC109cETsjogN2fJLwHbgrC5XaGZmNanlHv21krZkt3ZO6ayjpAZgFPBYDe9nZmY55A36RcB5QCOwG7itXEdJA4H7gL+MiN920m+WpGZJza2trTnLMjOzjnIFfUQ8HxGHIuJ14A5gTKl+kvpRDPm7I2JVhWMujohCRBTq6urylGVmZiXkCnpJZ7ZbnQpsK9FHwFJge0T8fb7yzMysVtU8XrkCWAsMk9QiaQZws6StkrYAfwp8Mes7WFLbEzhjgU8DEzp7DNPMzI6uvpU6RMS0Es1Ly/R9DpicLf8IUE3VmZlZzfzNWDOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwSV81vxi6TtEfStnZt8yXtqvRbsJImSXpS0g5Jc7uzcDMzq041V/TLgUkl2hdERGP2erDjRkl9gH8APgQMB6ZJGl5LsWZm1nXV/Dj4GkkNOY49BtgREc8ASLoH+Ajws0o7vvqrHWy45S9yvKWZmXVUMeg7ca2kK4Fm4EsR8esO288CftluvQX4k3IHkzQLmAUw4sz+vHP/zhpKMzOzNnmDfhHwN0Bkf28DrurQRyX2i3IHjIjFwGKAQqEQDTc05yzNzKwXurFU5BbleuomIp6PiEMR8TpwB8XbNB21AGe3W68HnsvzfmZmll+uoJd0ZrvVqcC2Et3WAe+WNFTSicAVwHfyvJ+ZmeVX8daNpBXAeGCQpBbgRmC8pEaKt2J2AldnfQcDSyJickQclHQt8BDQB1gWEY8flbMwM7OyFFH2tnmPKRQK0dzse/RmZtWStD4iCqW2+ZuxZmaJc9CbmSXOQW9mljgHvZlZ4hz0ZmaJc9CbmSXOQW9mljgHvZlZ4hz0ZmaJc9CbmSXOQW9mljgHvZlZ4hz0ZmaJc9CbmSXOQW9mljgHvZlZ4hz0ZmaJqyroJS2TtEfSm34bVtJsSSFpUJl9b5b0uKTtkm6XVP6nys3MrNtVe0W/HJjUsVHS2cAHgWdL7STpImAsMBIYAYwGLslTqJmZ5VNV0EfEGuCFEpsWANdT/JHwkrsC/YETgZOAfsDzXS/TzMzyyn2PXtIUYFdEbC7XJyLWAj8EdmevhyJie5njzZLULKm5tbU1b1lmZtZBrqCXNACYB9xQod+7gPOBeuAsYIKkcaX6RsTiiChERKGuri5PWWZmVkLeK/rzgKHAZkk7KQb5BklndOg3FfhpRLwcES8D3wMuzFusmZl1Xa6gj4itEXFaRDRERAPQAjRFxK86dH0WuERSX0n9KH4QW/LWjZmZHR3VPl65AlgLDJPUImlGJ30LkpZkqyuBnwNbgc3A5oi4v8aazcysC/pW0ykiplXY3tBuuRmYmS0fAq6uoT4zM6uRvxlrZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpa4ikEvaZmkPZK2ldg2W1JIGlRm33MkPSxpu6SfSWqovWQzM+uKaq7olwOTOjZKOhv4IMUfAC/nTuCWiDgfGAPsyVGjmZnVoGLQR8Qa4IUSmxYA1wNRaj9Jw4G+EfFIdpyXI+LVGmo1M7Mcct2jlzQF2BURmzvp9kfAbyStkrRR0i2S+uSq0szMcuty0EsaAMwDbqjQtS9wMTAbGA2cC3y2k+POktQsqbm1tbWrZZmZWRl5rujPA4YCmyXtBOqBDZLO6NCvBdgYEc9ExEHg20BTuYNGxOKIKEREoa6uLkdZZmZWSt+u7hARW4HT2tazsC9ExN4OXdcBp0iqi4hWYALQXEOtZmaWQzWPV64A1gLDJLVImtFJ34KkJQARcYjibZv/lLQVEHBH95RtZmbVqnhFHxHTKmxvaLfcDMxst/4IMLKG+szMrEb+ZqyZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiasq6CUtk7RH0rYS22ZLCkmDOtn/ZEm7JC2spVgzM+u6aq/olwOTOjZKOhv4IPBshf3/Bvh/XarMzMy6RVVBHxFrgBdKbFoAXA9EuX0lvQ84HXg4T4FmZlab3PfoJU0BdkXE5k76nADcBsyp4nizJDVLam5tbc1blpmZdZAr6CUNAOYBN1Toeg3wYET8stIxI2JxRBQiolBXV5enLDMzK6Fvzv3OA4YCmyUB1AMbJI2JiF+16/d+4GJJ1wADgRMlvRwRc2sp2szMqpcr6CNiK3Ba27qknUAhIvZ26De9XZ/PZn0c8mZmb6FqH69cAawFhklqkTSjk74FSUu6q0AzM6uNIso+MNNjCoVCNDc393QZZmbHDUnrI6JQapu/GWtmljgHvZlZ4hz0ZmaJc9CbmSXOQW9mljgHvZlZ4hz0ZmaJc9CbmSXOQW9mljgHvZlZ4hz0ZmaJc9CbmSXOQW9mljgHvZlZ4hz0ZmaJc9CbmSXOQW9mlriKQS9pmaQ9kraV2DZbUkgaVGJbo6S1kh6XtEXSJ7uraDMzq141V/TLgUkdGyWdDXwQeLbMfq8CV0bEH2f7f03SO3LWaWZmOVUM+ohYA7xQYtMC4Hqg5I/ORsRTEfF0tvwcsAeoy1+qmZnlkesevaQpwK6I2Fxl/zHAicDP87yfmZnl17erO0gaAMwDLquy/5nAXcBnIuL1TvrNAmYBnHPOOV0ty8zMyshzRX8eMBTYLGknUA9skHRGx46STga+C/yfiPhpZweNiMURUYiIQl2d7/CYmXWXLl/RR8RW4LS29SzsCxGxt30/SScC3wLujIh/r7FOMzPLSRElP0s90kFaAYwHBgHPAzdGxNJ223eSBb2kAvCFiJgp6VPAPwGPtzvcZyNiU8WipJeAJ7t4LqkaBOyt2Kt38FgUeRyO8FgcMSQiSt4OqRj0PUFSc0QUerqOY4HH4giPRZHH4QiPRXX8zVgzs8Q56M3MEnesBv3ini7gGOKxOMJjUeRxOMJjUYVj8h69mZl1n2P1it7MzLrJMRX0kiZJelLSDklze7qeo6HUbKCS3inpEUlPZ39Pydol6fZsPLZIamq3z2ey/k9L+kxPnEutJJ0t6YeStmeznF6Xtfe68ZDUX9J/SdqcjcVNWftQSY9l53Vv9v0UJJ2Ure/Itje0O9aXs/YnJU3smTOqjaQ+kjZKeiBb75Xj0G0i4ph4AX0ozoVzLsV5cTYDw3u6rqNwnuOAJmBbu7abgbnZ8lzg77LlycD3AAEXAo9l7e8Ensn+npItn9LT55ZjLM4EmrLltwFPAcN743hk5zQwW+4HPJad478BV2Tt/wj8j2z5GuAfs+UrgHuz5eHZfzsnUfwG+8+BPj19fjnG46+AfwUeyNZ75Th01+tYuqIfA+yIiGci4vfAPcBHerimbhelZwP9CPDP2fI/Ax9t135nFP0UeEc2d9BE4JGIeCEifg08QomppI91EbE7IjZkyy8B24Gz6IXjkZ3Ty9lqv+wVwARgZdbecSzaxmgl8GeSlLXfExG/i4j/BnZQ/G/ruCGpHvhzYEm2LnrhOHSnYynozwJ+2W69JWvrDU6PiN1QDD+OTDFRbkySG6vsn9yjKF7J9srxyG5XbKI4pfcjFK9CfxMRB7Mu7c/r8Dln218ETiWNsfgaxSnQ2yZBPJXeOQ7d5lgKepVo6+2PBJUbk6TGStJA4D7gLyPit511LdGWzHhExKGIaKQ4UeAY4PxS3bK/SY6FpA8DeyJiffvmEl2THofudiwFfQtwdrv1euC5HqrlrfZ8dguibVrnPVl7uTFJZqwk9aMY8ndHxKqsudeOB0BE/AZYTfEe/TsktU0+2P68Dp9ztv3tFG8JHu9jMRaYks2hdQ/FWzZfo/eNQ7c6loJ+HfDu7NP1Eyl+sPKdHq7prfIdoO1Jkc8A/9Gu/crsaZMLgRezWxkPAZdJOiV7IuWyrO24kt1LXQpsj4i/b7ep142HpDplP7Up6Q+ASyl+ZvFD4PKsW8exaBujy4EfRPFTyO8AV2RPowwF3g3811tzFrWLiC9HRH1ENFDMgB9ExHR62Th0u57+NLj9i+JTFU9RvDc5r6frOUrnuALYDRygeNUxg+I9xf8Ens7+vjPrK+AfsvHYSnGW0LbjXEXxA6YdwOd6+rxyjsUHKP5zeguwKXtN7o3jAYwENmZjsQ24IWs/l2JA7QD+HTgpa++fre/Itp/b7ljzsjF6EvhQT59bDWMyniNP3fTaceiOl78Za2aWuGPp1o2ZmR0FDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNL3P8HHqk1UKaWHdIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## implement classification using 2 inputs 1 output.\n",
    "lengths = {\"question1\":[],\"question2\":[]}\n",
    "for k in range(len(final_data[\"question1\"])):\n",
    "    lengths[\"question1\"].append(len(final_data[\"question1\"][k]))\n",
    "    lengths[\"question2\"].append(len(final_data[\"question2\"][k]))\n",
    "    \n",
    "kk = pd.DataFrame(lengths)\n",
    "kk.plot()\n",
    "print(len(kk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Train test Split\n",
    "hold = []\n",
    "for k in range(len(final_data[\"question1\"])):\n",
    "    hold.append(final_data[\"question1\"][k]+final_data[\"question2\"][k])\n",
    "len(hold[0])\n",
    "final_data[\"combined\"] = hold\n",
    "del hold\n",
    "x_train,x_test,y_train,y_test = tts(final_data[\"combined\"],final_data[\"label\"],test_size=0.3)\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_train= lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'story', 'of', 'kohinoor', 'kohinoor', 'diamond', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'] - Length: 15\n",
      "['what', 'would', 'happen', 'if', 'the', 'indian', 'government', 'stole', 'the', 'kohinoor', 'kohinoor', 'diamond', 'back', 'PAD', 'PAD'] - Length: 15\n",
      "Examples: 1000\n"
     ]
    }
   ],
   "source": [
    "BertVecs = Bert()\n",
    "\n",
    "####Truncade and Pad to 20 words max\n",
    "sentences1 = list(map(wordTok,data[\"question1\"]))\n",
    "sentences2 = list(map(wordTok,data[\"question2\"]))\n",
    "\n",
    "sentences1 = list(map(list,sequence_padding(sentences1)))\n",
    "sentences2 = list(map(list,sequence_padding(sentences2)))\n",
    "\n",
    "print(sentences1[1],\"- Length: %s\"%len(sentences1[1]))\n",
    "print(sentences2[1],\"- Length: %s\"%len(sentences2[1]))\n",
    "\n",
    "#back to sentences \n",
    "sentences1 = list(map(list2string,sentences1))\n",
    "sentences2 = list(map(list2string,sentences2))\n",
    "print(\"Examples: %s\"%len(sentences2))\n",
    "results1 = BertVecs(sentences1)\n",
    "results2 = BertVecs(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n",
      "15 15\n",
      "Final data\n",
      "999 999\n"
     ]
    }
   ],
   "source": [
    "data1 = [results1[k][1] for k in range(len(results1))]\n",
    "data2 = [results2[k][1] for k in range(len(results2))]\n",
    "len(data1[6])\n",
    "#del sentences1\n",
    "#del sentences2\n",
    "\n",
    "dataall=[]\n",
    "labels =[]\n",
    "for k in range(len(data1)):\n",
    "    if len(data1[k])>STRING_SIZE or len(data1[k])>STRING_SIZE:\n",
    "        print(k)\n",
    "        data1[k] = data1[k][:STRING_SIZE]\n",
    "        data2[k] = data2[k][:STRING_SIZE]\n",
    "        print(len(data1[k]),len(data2[k]))\n",
    "    try:\n",
    "        if len(data1[k])==STRING_SIZE and len(data2[k])==STRING_SIZE:\n",
    "            dataall.append(np.array(data1[k]+data2[k]))\n",
    "            labels.append(data[\"is_duplicate\"][k])\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        print(\"cant get \",k)\n",
    "        print(\"shape : \",np.array(data1[k]).shape)\n",
    "print(\"Final data\")\n",
    "print(len(dataall),len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 30, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dataall).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Clear memory befor training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del results1\n",
    "del results2\n",
    "del BertVecs\n",
    "#del data[\"question1\"]\n",
    "#del data[\"question2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= to_categorical(labels)\n",
    "x_train,x_test,y_train,y_test = tts(dataall,y,test_size=0.2)\n",
    "x_train,x_val,y_train,y_val = tts(x_train,y_train,test_size=0.1)\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "x_val = np.asarray(x_val)\n",
    "x_test = np.asarray(x_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_val = np.asarray(y_val)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(x_train)\n",
    "VAL_SIZE = len(x_val)\n",
    "\n",
    "\n",
    "gentrain = get_samples(x_train,y_train,BATCH_SIZE)\n",
    "gentest = get_samples(x_val,y_val,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(719, 30, 768)\n",
      "(719, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the embedding are returned as a list of sentences and their token embeddings, so each sentence is of the form<br>\n",
    "__(sentence,embeddings for each word)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Build an LSTM model with 2 inputs and one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - ETA: 56s - loss: 0.7500 - accuracy: 0.333 - ETA: 28s - loss: 0.6641 - accuracy: 0.500 - ETA: 18s - loss: 0.5608 - accuracy: 0.666 - ETA: 13s - loss: 0.5614 - accuracy: 0.666 - ETA: 10s - loss: 0.6265 - accuracy: 0.666 - ETA: 8s - loss: 0.7901 - accuracy: 0.611 - ETA: 7s - loss: 0.8973 - accuracy: 0.52 - ETA: 6s - loss: 0.8429 - accuracy: 0.54 - ETA: 5s - loss: 0.8349 - accuracy: 0.55 - ETA: 4s - loss: 0.8722 - accuracy: 0.53 - ETA: 4s - loss: 0.8076 - accuracy: 0.57 - ETA: 3s - loss: 0.8508 - accuracy: 0.58 - ETA: 3s - loss: 1.1120 - accuracy: 0.56 - ETA: 2s - loss: 1.0813 - accuracy: 0.57 - ETA: 2s - loss: 1.1353 - accuracy: 0.57 - ETA: 2s - loss: 1.5415 - accuracy: 0.56 - ETA: 1s - loss: 1.5044 - accuracy: 0.56 - ETA: 1s - loss: 1.4738 - accuracy: 0.57 - ETA: 1s - loss: 1.4194 - accuracy: 0.57 - ETA: 1s - loss: 1.3644 - accuracy: 0.60 - ETA: 0s - loss: 1.4445 - accuracy: 0.60 - ETA: 0s - loss: 1.4525 - accuracy: 0.59 - ETA: 0s - loss: 1.4189 - accuracy: 0.59 - ETA: 0s - loss: 1.3772 - accuracy: 0.61 - ETA: 0s - loss: 1.3487 - accuracy: 0.61 - ETA: 0s - loss: 1.3473 - accuracy: 0.60 - 5s 58ms/step - loss: 1.3578 - accuracy: 0.5875 - val_loss: 0.6563 - val_accuracy: 0.6500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x139806feda0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64,return_sequences=True,activation='relu',dropout=0.3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64,return_sequences=False,activation='relu',dropout=0.3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(\"adam\", loss ='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Training...')\n",
    "model.fit(x_train[:100], y_train[:100],\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          batch_size=3,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - ETA: 4s - loss: 1.2250 - accuracy: 0.50 - ETA: 3s - loss: 1.1633 - accuracy: 0.50 - ETA: 3s - loss: 0.9643 - accuracy: 0.57 - ETA: 2s - loss: 0.9298 - accuracy: 0.57 - ETA: 2s - loss: 0.8830 - accuracy: 0.57 - ETA: 2s - loss: 0.8959 - accuracy: 0.56 - ETA: 2s - loss: 0.8918 - accuracy: 0.55 - ETA: 1s - loss: 0.8907 - accuracy: 0.55 - ETA: 1s - loss: 0.8733 - accuracy: 0.56 - ETA: 1s - loss: 0.8624 - accuracy: 0.55 - ETA: 1s - loss: 0.8449 - accuracy: 0.56 - ETA: 1s - loss: 0.8315 - accuracy: 0.56 - ETA: 1s - loss: 0.8182 - accuracy: 0.57 - ETA: 1s - loss: 0.8136 - accuracy: 0.56 - ETA: 0s - loss: 0.8025 - accuracy: 0.56 - ETA: 0s - loss: 0.7962 - accuracy: 0.56 - ETA: 0s - loss: 0.7887 - accuracy: 0.56 - ETA: 0s - loss: 0.7785 - accuracy: 0.57 - ETA: 0s - loss: 0.7769 - accuracy: 0.57 - ETA: 0s - loss: 0.7950 - accuracy: 0.56 - ETA: 0s - loss: 0.7914 - accuracy: 0.56 - 3s 137ms/step - loss: 0.7840 - accuracy: 0.5781 - val_loss: 0.6803 - val_accuracy: 0.6116\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.6992 - accuracy: 0.62 - ETA: 2s - loss: 0.7395 - accuracy: 0.48 - ETA: 2s - loss: 0.7116 - accuracy: 0.54 - ETA: 2s - loss: 0.7122 - accuracy: 0.53 - ETA: 2s - loss: 0.7113 - accuracy: 0.53 - ETA: 1s - loss: 0.7018 - accuracy: 0.54 - ETA: 1s - loss: 0.6851 - accuracy: 0.56 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6816 - accuracy: 0.57 - ETA: 1s - loss: 0.6813 - accuracy: 0.57 - ETA: 1s - loss: 0.6772 - accuracy: 0.58 - ETA: 1s - loss: 0.6723 - accuracy: 0.58 - ETA: 0s - loss: 0.6674 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.58 - ETA: 0s - loss: 0.6686 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6671 - accuracy: 0.60 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.60 - 3s 134ms/step - loss: 0.6676 - accuracy: 0.6108 - val_loss: 0.6069 - val_accuracy: 0.6875\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.6258 - accuracy: 0.65 - ETA: 2s - loss: 0.6470 - accuracy: 0.62 - ETA: 2s - loss: 0.6539 - accuracy: 0.62 - ETA: 2s - loss: 0.6800 - accuracy: 0.56 - ETA: 2s - loss: 0.6773 - accuracy: 0.55 - ETA: 1s - loss: 0.6786 - accuracy: 0.54 - ETA: 1s - loss: 0.6821 - accuracy: 0.54 - ETA: 1s - loss: 0.6688 - accuracy: 0.57 - ETA: 1s - loss: 0.6678 - accuracy: 0.56 - ETA: 1s - loss: 0.6696 - accuracy: 0.56 - ETA: 1s - loss: 0.6683 - accuracy: 0.57 - ETA: 1s - loss: 0.6667 - accuracy: 0.57 - ETA: 1s - loss: 0.6643 - accuracy: 0.57 - ETA: 0s - loss: 0.6639 - accuracy: 0.57 - ETA: 0s - loss: 0.6651 - accuracy: 0.58 - ETA: 0s - loss: 0.6570 - accuracy: 0.59 - ETA: 0s - loss: 0.6589 - accuracy: 0.59 - ETA: 0s - loss: 0.6594 - accuracy: 0.58 - ETA: 0s - loss: 0.6630 - accuracy: 0.58 - ETA: 0s - loss: 0.6620 - accuracy: 0.59 - ETA: 0s - loss: 0.6623 - accuracy: 0.59 - 3s 136ms/step - loss: 0.6635 - accuracy: 0.5923 - val_loss: 0.7393 - val_accuracy: 0.5804\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.6588 - accuracy: 0.68 - ETA: 2s - loss: 0.6419 - accuracy: 0.65 - ETA: 2s - loss: 0.7006 - accuracy: 0.59 - ETA: 2s - loss: 0.6877 - accuracy: 0.60 - ETA: 2s - loss: 0.6754 - accuracy: 0.62 - ETA: 2s - loss: 0.6701 - accuracy: 0.63 - ETA: 1s - loss: 0.6692 - accuracy: 0.61 - ETA: 1s - loss: 0.6662 - accuracy: 0.61 - ETA: 1s - loss: 0.6594 - accuracy: 0.61 - ETA: 1s - loss: 0.6538 - accuracy: 0.61 - ETA: 1s - loss: 0.6557 - accuracy: 0.61 - ETA: 1s - loss: 0.6557 - accuracy: 0.60 - ETA: 1s - loss: 0.6531 - accuracy: 0.61 - ETA: 1s - loss: 0.6644 - accuracy: 0.59 - ETA: 0s - loss: 0.6607 - accuracy: 0.60 - ETA: 0s - loss: 0.6613 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.60 - ETA: 0s - loss: 0.6595 - accuracy: 0.60 - ETA: 0s - loss: 0.6569 - accuracy: 0.61 - ETA: 0s - loss: 0.6567 - accuracy: 0.60 - ETA: 0s - loss: 0.6592 - accuracy: 0.60 - 3s 138ms/step - loss: 0.6596 - accuracy: 0.6023 - val_loss: 0.6785 - val_accuracy: 0.6205\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.7007 - accuracy: 0.59 - ETA: 2s - loss: 0.6792 - accuracy: 0.60 - ETA: 2s - loss: 0.6750 - accuracy: 0.60 - ETA: 2s - loss: 0.6786 - accuracy: 0.57 - ETA: 2s - loss: 0.6812 - accuracy: 0.56 - ETA: 2s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6823 - accuracy: 0.55 - ETA: 1s - loss: 0.6822 - accuracy: 0.56 - ETA: 1s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.54 - ETA: 1s - loss: 0.6834 - accuracy: 0.55 - ETA: 1s - loss: 0.6853 - accuracy: 0.54 - ETA: 1s - loss: 0.6825 - accuracy: 0.54 - ETA: 1s - loss: 0.6796 - accuracy: 0.55 - ETA: 0s - loss: 0.6752 - accuracy: 0.56 - ETA: 0s - loss: 0.6735 - accuracy: 0.56 - ETA: 0s - loss: 0.6700 - accuracy: 0.57 - ETA: 0s - loss: 0.6677 - accuracy: 0.58 - ETA: 0s - loss: 0.6695 - accuracy: 0.57 - ETA: 0s - loss: 0.6712 - accuracy: 0.57 - ETA: 0s - loss: 0.6712 - accuracy: 0.57 - 3s 145ms/step - loss: 0.6716 - accuracy: 0.5710 - val_loss: 0.6556 - val_accuracy: 0.6518\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.6445 - accuracy: 0.62 - ETA: 2s - loss: 0.6656 - accuracy: 0.59 - ETA: 2s - loss: 0.6401 - accuracy: 0.63 - ETA: 2s - loss: 0.6413 - accuracy: 0.60 - ETA: 1s - loss: 0.6456 - accuracy: 0.61 - ETA: 1s - loss: 0.6422 - accuracy: 0.62 - ETA: 1s - loss: 0.6431 - accuracy: 0.60 - ETA: 1s - loss: 0.6370 - accuracy: 0.60 - ETA: 1s - loss: 0.6260 - accuracy: 0.61 - ETA: 1s - loss: 0.6277 - accuracy: 0.61 - ETA: 1s - loss: 0.6287 - accuracy: 0.61 - ETA: 1s - loss: 0.6316 - accuracy: 0.61 - ETA: 1s - loss: 0.6273 - accuracy: 0.62 - ETA: 0s - loss: 0.6204 - accuracy: 0.63 - ETA: 0s - loss: 0.6157 - accuracy: 0.63 - ETA: 0s - loss: 0.6167 - accuracy: 0.63 - ETA: 0s - loss: 0.6221 - accuracy: 0.62 - ETA: 0s - loss: 0.6230 - accuracy: 0.62 - ETA: 0s - loss: 0.6236 - accuracy: 0.62 - ETA: 0s - loss: 0.6252 - accuracy: 0.63 - ETA: 0s - loss: 0.6208 - accuracy: 0.63 - 3s 137ms/step - loss: 0.6217 - accuracy: 0.6278 - val_loss: 0.6671 - val_accuracy: 0.6116\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.6461 - accuracy: 0.65 - ETA: 2s - loss: 0.6473 - accuracy: 0.67 - ETA: 2s - loss: 0.6586 - accuracy: 0.64 - ETA: 2s - loss: 0.6456 - accuracy: 0.64 - ETA: 2s - loss: 0.6305 - accuracy: 0.64 - ETA: 1s - loss: 0.6326 - accuracy: 0.63 - ETA: 1s - loss: 0.6392 - accuracy: 0.62 - ETA: 1s - loss: 0.6437 - accuracy: 0.63 - ETA: 1s - loss: 0.6473 - accuracy: 0.63 - ETA: 1s - loss: 0.6546 - accuracy: 0.62 - ETA: 1s - loss: 0.6498 - accuracy: 0.62 - ETA: 1s - loss: 0.6479 - accuracy: 0.63 - ETA: 1s - loss: 0.6494 - accuracy: 0.62 - ETA: 0s - loss: 0.6461 - accuracy: 0.62 - ETA: 0s - loss: 0.6456 - accuracy: 0.62 - ETA: 0s - loss: 0.6403 - accuracy: 0.62 - ETA: 0s - loss: 0.6393 - accuracy: 0.62 - ETA: 0s - loss: 0.6442 - accuracy: 0.61 - ETA: 0s - loss: 0.6457 - accuracy: 0.61 - ETA: 0s - loss: 0.6383 - accuracy: 0.61 - ETA: 0s - loss: 0.6395 - accuracy: 0.61 - 3s 136ms/step - loss: 0.6368 - accuracy: 0.6207 - val_loss: 0.7518 - val_accuracy: 0.6473\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.7368 - accuracy: 0.65 - ETA: 2s - loss: 0.6366 - accuracy: 0.68 - ETA: 2s - loss: 0.6454 - accuracy: 0.66 - ETA: 2s - loss: 0.6500 - accuracy: 0.63 - ETA: 2s - loss: 0.6331 - accuracy: 0.64 - ETA: 2s - loss: 0.6444 - accuracy: 0.60 - ETA: 1s - loss: 0.6446 - accuracy: 0.61 - ETA: 1s - loss: 0.6406 - accuracy: 0.61 - ETA: 1s - loss: 0.6359 - accuracy: 0.61 - ETA: 1s - loss: 0.6375 - accuracy: 0.60 - ETA: 1s - loss: 0.6407 - accuracy: 0.59 - ETA: 1s - loss: 0.6434 - accuracy: 0.59 - ETA: 1s - loss: 0.6459 - accuracy: 0.58 - ETA: 0s - loss: 0.6475 - accuracy: 0.57 - ETA: 0s - loss: 0.6475 - accuracy: 0.57 - ETA: 0s - loss: 0.6464 - accuracy: 0.58 - ETA: 0s - loss: 0.6494 - accuracy: 0.58 - ETA: 0s - loss: 0.6455 - accuracy: 0.58 - ETA: 0s - loss: 0.6441 - accuracy: 0.59 - ETA: 0s - loss: 0.6471 - accuracy: 0.58 - ETA: 0s - loss: 0.6406 - accuracy: 0.59 - 3s 153ms/step - loss: 0.6441 - accuracy: 0.5895 - val_loss: 0.7549 - val_accuracy: 0.5714\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.6074 - accuracy: 0.62 - ETA: 2s - loss: 0.5979 - accuracy: 0.64 - ETA: 2s - loss: 0.6016 - accuracy: 0.64 - ETA: 2s - loss: 0.6055 - accuracy: 0.61 - ETA: 2s - loss: 0.5898 - accuracy: 0.65 - ETA: 2s - loss: 0.6027 - accuracy: 0.64 - ETA: 1s - loss: 0.6084 - accuracy: 0.62 - ETA: 1s - loss: 0.6189 - accuracy: 0.60 - ETA: 1s - loss: 0.6164 - accuracy: 0.60 - ETA: 1s - loss: 0.6132 - accuracy: 0.60 - ETA: 1s - loss: 0.6325 - accuracy: 0.59 - ETA: 1s - loss: 0.6298 - accuracy: 0.59 - ETA: 1s - loss: 0.6293 - accuracy: 0.59 - ETA: 0s - loss: 0.6288 - accuracy: 0.59 - ETA: 0s - loss: 0.6272 - accuracy: 0.60 - ETA: 0s - loss: 0.6259 - accuracy: 0.59 - ETA: 0s - loss: 0.6287 - accuracy: 0.59 - ETA: 0s - loss: 0.6307 - accuracy: 0.59 - ETA: 0s - loss: 0.6264 - accuracy: 0.60 - ETA: 0s - loss: 0.6243 - accuracy: 0.60 - ETA: 0s - loss: 0.6237 - accuracy: 0.60 - 3s 150ms/step - loss: 0.6240 - accuracy: 0.6023 - val_loss: 0.4916 - val_accuracy: 0.6920\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.5745 - accuracy: 0.71 - ETA: 2s - loss: 0.5913 - accuracy: 0.68 - ETA: 2s - loss: 0.5899 - accuracy: 0.66 - ETA: 1s - loss: 0.5810 - accuracy: 0.67 - ETA: 1s - loss: 0.5957 - accuracy: 0.63 - ETA: 1s - loss: 0.6393 - accuracy: 0.63 - ETA: 1s - loss: 0.6303 - accuracy: 0.63 - ETA: 1s - loss: 0.6337 - accuracy: 0.63 - ETA: 1s - loss: 0.6228 - accuracy: 0.65 - ETA: 1s - loss: 0.6227 - accuracy: 0.65 - ETA: 1s - loss: 0.6247 - accuracy: 0.64 - ETA: 1s - loss: 0.6206 - accuracy: 0.64 - ETA: 1s - loss: 0.6189 - accuracy: 0.64 - ETA: 0s - loss: 0.6137 - accuracy: 0.65 - ETA: 0s - loss: 0.6117 - accuracy: 0.64 - ETA: 0s - loss: 0.6082 - accuracy: 0.64 - ETA: 0s - loss: 0.6029 - accuracy: 0.65 - ETA: 0s - loss: 0.6059 - accuracy: 0.64 - ETA: 0s - loss: 0.6028 - accuracy: 0.65 - ETA: 0s - loss: 0.6025 - accuracy: 0.65 - ETA: 0s - loss: 0.5999 - accuracy: 0.64 - 3s 145ms/step - loss: 0.6013 - accuracy: 0.6548 - val_loss: 0.9682 - val_accuracy: 0.6116\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.6146 - accuracy: 0.65 - ETA: 2s - loss: 0.6036 - accuracy: 0.65 - ETA: 2s - loss: 0.6121 - accuracy: 0.63 - ETA: 2s - loss: 0.6312 - accuracy: 0.61 - ETA: 1s - loss: 0.7099 - accuracy: 0.59 - ETA: 1s - loss: 0.6916 - accuracy: 0.60 - ETA: 1s - loss: 0.6734 - accuracy: 0.62 - ETA: 1s - loss: 0.6635 - accuracy: 0.63 - ETA: 1s - loss: 0.6585 - accuracy: 0.63 - ETA: 1s - loss: 0.6506 - accuracy: 0.64 - ETA: 1s - loss: 0.6502 - accuracy: 0.62 - ETA: 1s - loss: 0.6436 - accuracy: 0.62 - ETA: 1s - loss: 0.6458 - accuracy: 0.61 - ETA: 0s - loss: 0.6453 - accuracy: 0.60 - ETA: 0s - loss: 0.6386 - accuracy: 0.61 - ETA: 0s - loss: 0.6415 - accuracy: 0.61 - ETA: 0s - loss: 0.6407 - accuracy: 0.61 - ETA: 0s - loss: 0.6389 - accuracy: 0.61 - ETA: 0s - loss: 0.6385 - accuracy: 0.61 - ETA: 0s - loss: 0.6343 - accuracy: 0.62 - ETA: 0s - loss: 0.6327 - accuracy: 0.62 - 3s 132ms/step - loss: 0.6334 - accuracy: 0.6222 - val_loss: 0.7182 - val_accuracy: 0.6652\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.6591 - accuracy: 0.56 - ETA: 2s - loss: 0.6702 - accuracy: 0.62 - ETA: 2s - loss: 0.6355 - accuracy: 0.65 - ETA: 2s - loss: 0.6163 - accuracy: 0.66 - ETA: 2s - loss: 0.6009 - accuracy: 0.66 - ETA: 1s - loss: 0.6002 - accuracy: 0.68 - ETA: 1s - loss: 0.6156 - accuracy: 0.66 - ETA: 1s - loss: 0.6120 - accuracy: 0.66 - ETA: 1s - loss: 0.6126 - accuracy: 0.66 - ETA: 1s - loss: 0.6057 - accuracy: 0.66 - ETA: 1s - loss: 0.6049 - accuracy: 0.66 - ETA: 1s - loss: 0.6015 - accuracy: 0.67 - ETA: 1s - loss: 0.5958 - accuracy: 0.67 - ETA: 1s - loss: 0.5974 - accuracy: 0.66 - ETA: 0s - loss: 0.6052 - accuracy: 0.66 - ETA: 0s - loss: 0.6072 - accuracy: 0.65 - ETA: 0s - loss: 0.6059 - accuracy: 0.65 - ETA: 0s - loss: 0.6080 - accuracy: 0.64 - ETA: 0s - loss: 0.6037 - accuracy: 0.64 - ETA: 0s - loss: 0.6045 - accuracy: 0.64 - ETA: 0s - loss: 0.6392 - accuracy: 0.64 - 3s 135ms/step - loss: 0.6451 - accuracy: 0.6449 - val_loss: 0.8922 - val_accuracy: 0.6652\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.4686 - accuracy: 0.71 - ETA: 2s - loss: 0.5740 - accuracy: 0.71 - ETA: 2s - loss: 0.6387 - accuracy: 0.71 - ETA: 2s - loss: 0.8895 - accuracy: 0.65 - ETA: 1s - loss: 0.9030 - accuracy: 0.63 - ETA: 1s - loss: 0.9254 - accuracy: 0.61 - ETA: 1s - loss: 1.1310 - accuracy: 0.59 - ETA: 1s - loss: 1.4280 - accuracy: 0.58 - ETA: 1s - loss: 1.8095 - accuracy: 0.58 - ETA: 1s - loss: 1.8207 - accuracy: 0.59 - ETA: 1s - loss: 1.8378 - accuracy: 0.59 - ETA: 1s - loss: 1.9294 - accuracy: 0.58 - ETA: 1s - loss: 1.9127 - accuracy: 0.59 - ETA: 0s - loss: 1.9977 - accuracy: 0.58 - ETA: 0s - loss: 1.9936 - accuracy: 0.57 - ETA: 0s - loss: 1.9742 - accuracy: 0.57 - ETA: 0s - loss: 1.9926 - accuracy: 0.56 - ETA: 0s - loss: 1.9524 - accuracy: 0.56 - ETA: 0s - loss: 1.9817 - accuracy: 0.56 - ETA: 0s - loss: 2.0471 - accuracy: 0.55 - ETA: 0s - loss: 2.0209 - accuracy: 0.55 - 3s 137ms/step - loss: 2.0413 - accuracy: 0.5554 - val_loss: 8.3925 - val_accuracy: 0.3795\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 4.2098 - accuracy: 0.50 - ETA: 3s - loss: 4.4397 - accuracy: 0.51 - ETA: 3s - loss: 4.0001 - accuracy: 0.54 - ETA: 2s - loss: 4.5243 - accuracy: 0.53 - ETA: 2s - loss: 4.3961 - accuracy: 0.52 - ETA: 2s - loss: 4.3110 - accuracy: 0.51 - ETA: 2s - loss: 4.5041 - accuracy: 0.50 - ETA: 1s - loss: 4.9930 - accuracy: 0.51 - ETA: 1s - loss: 5.0571 - accuracy: 0.52 - ETA: 1s - loss: 5.1460 - accuracy: 0.52 - ETA: 1s - loss: 5.8052 - accuracy: 0.51 - ETA: 1s - loss: 6.1139 - accuracy: 0.52 - ETA: 1s - loss: 9.5835 - accuracy: 0.50 - ETA: 1s - loss: 10.5007 - accuracy: 0.502 - ETA: 0s - loss: 12.4025 - accuracy: 0.508 - ETA: 0s - loss: 12.4184 - accuracy: 0.511 - ETA: 0s - loss: 13.2310 - accuracy: 0.507 - ETA: 0s - loss: 13.8809 - accuracy: 0.506 - ETA: 0s - loss: 13.6168 - accuracy: 0.511 - ETA: 0s - loss: 13.3664 - accuracy: 0.509 - ETA: 0s - loss: 14.8291 - accuracy: 0.519 - 3s 142ms/step - loss: 15.0697 - accuracy: 0.5199 - val_loss: 5.5207 - val_accuracy: 0.6473\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 5.0902 - accuracy: 0.59 - ETA: 2s - loss: 9.9406 - accuracy: 0.50 - ETA: 2s - loss: 10.6436 - accuracy: 0.479 - ETA: 2s - loss: 12.9816 - accuracy: 0.460 - ETA: 1s - loss: 12.4984 - accuracy: 0.468 - ETA: 1s - loss: 13.3753 - accuracy: 0.474 - ETA: 1s - loss: 18.2992 - accuracy: 0.500 - ETA: 1s - loss: 16.7079 - accuracy: 0.503 - ETA: 1s - loss: 16.2815 - accuracy: 0.506 - ETA: 1s - loss: 26.9707 - accuracy: 0.506 - ETA: 1s - loss: 31.1747 - accuracy: 0.511 - ETA: 1s - loss: 32.0253 - accuracy: 0.502 - ETA: 0s - loss: 30.0000 - accuracy: 0.521 - ETA: 0s - loss: 36.0335 - accuracy: 0.513 - ETA: 0s - loss: 35.0483 - accuracy: 0.514 - ETA: 0s - loss: 33.0352 - accuracy: 0.521 - ETA: 0s - loss: 42.6572 - accuracy: 0.522 - ETA: 0s - loss: 46.3096 - accuracy: 0.522 - ETA: 0s - loss: 44.9950 - accuracy: 0.523 - ETA: 0s - loss: 90.6291 - accuracy: 0.520 - ETA: 0s - loss: 88.1308 - accuracy: 0.526 - 3s 129ms/step - loss: 119.9012 - accuracy: 0.5312 - val_loss: 149.5654 - val_accuracy: 0.4509\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 341.5858 - accuracy: 0.59 - ETA: 3s - loss: 380.6259 - accuracy: 0.59 - ETA: 3s - loss: 414.7954 - accuracy: 0.55 - ETA: 3s - loss: 629.2564 - accuracy: 0.53 - ETA: 2s - loss: 823.7174 - accuracy: 0.52 - ETA: 2s - loss: 813.4428 - accuracy: 0.54 - ETA: 2s - loss: 888.0112 - accuracy: 0.53 - ETA: 2s - loss: 1498.5158 - accuracy: 0.527 - ETA: 2s - loss: 1408.2712 - accuracy: 0.527 - ETA: 1s - loss: 1359.3434 - accuracy: 0.521 - ETA: 1s - loss: 1262.7171 - accuracy: 0.525 - ETA: 1s - loss: 1272.3248 - accuracy: 0.523 - ETA: 1s - loss: 1695.3722 - accuracy: 0.514 - ETA: 1s - loss: 1631.1341 - accuracy: 0.513 - ETA: 1s - loss: 1605.5963 - accuracy: 0.520 - ETA: 0s - loss: 1550.9157 - accuracy: 0.523 - ETA: 0s - loss: 1534.7929 - accuracy: 0.518 - ETA: 0s - loss: 1548.8082 - accuracy: 0.519 - ETA: 0s - loss: 1538.2570 - accuracy: 0.513 - ETA: 0s - loss: 1588.6864 - accuracy: 0.510 - ETA: 0s - loss: 1636.6424 - accuracy: 0.508 - 4s 160ms/step - loss: 1778.7467 - accuracy: 0.5142 - val_loss: 683.7914 - val_accuracy: 0.3929\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1272.6414 - accuracy: 0.468 - ETA: 2s - loss: 3036.0873 - accuracy: 0.562 - ETA: 2s - loss: 2590.6555 - accuracy: 0.552 - ETA: 2s - loss: 2706.3564 - accuracy: 0.562 - ETA: 2s - loss: 2583.3321 - accuracy: 0.556 - ETA: 2s - loss: 2285.2040 - accuracy: 0.562 - ETA: 2s - loss: 2007.4335 - accuracy: 0.558 - ETA: 1s - loss: 1814.2897 - accuracy: 0.562 - ETA: 1s - loss: 1839.3089 - accuracy: 0.566 - ETA: 1s - loss: 1851.3246 - accuracy: 0.553 - ETA: 1s - loss: 3174.9646 - accuracy: 0.539 - ETA: 1s - loss: 3575.3437 - accuracy: 0.539 - ETA: 1s - loss: 3626.4607 - accuracy: 0.531 - ETA: 1s - loss: 3420.5378 - accuracy: 0.533 - ETA: 0s - loss: 3462.7004 - accuracy: 0.537 - ETA: 0s - loss: 3550.6438 - accuracy: 0.535 - ETA: 0s - loss: 4006.1251 - accuracy: 0.533 - ETA: 0s - loss: 3943.6809 - accuracy: 0.538 - ETA: 0s - loss: 4054.6730 - accuracy: 0.532 - ETA: 0s - loss: 3988.2764 - accuracy: 0.539 - ETA: 0s - loss: 4008.9897 - accuracy: 0.535 - 3s 144ms/step - loss: 3870.9844 - accuracy: 0.5426 - val_loss: 19.2761 - val_accuracy: 0.5446\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 161.1446 - accuracy: 0.56 - ETA: 2s - loss: 475.5521 - accuracy: 0.50 - ETA: 2s - loss: 1475.3371 - accuracy: 0.447 - ETA: 2s - loss: 1317.5866 - accuracy: 0.460 - ETA: 2s - loss: 1120.8568 - accuracy: 0.468 - ETA: 2s - loss: 1218.6707 - accuracy: 0.463 - ETA: 1s - loss: 1057.4056 - accuracy: 0.464 - ETA: 1s - loss: 1185.9978 - accuracy: 0.449 - ETA: 1s - loss: 1218.9536 - accuracy: 0.441 - ETA: 1s - loss: 1180.2374 - accuracy: 0.434 - ETA: 1s - loss: 1102.1685 - accuracy: 0.440 - ETA: 1s - loss: 1032.9875 - accuracy: 0.447 - ETA: 1s - loss: 997.0760 - accuracy: 0.451 - ETA: 1s - loss: 934.1135 - accuracy: 0.45 - ETA: 0s - loss: 914.6406 - accuracy: 0.46 - ETA: 0s - loss: 902.5883 - accuracy: 0.45 - ETA: 0s - loss: 853.5997 - accuracy: 0.45 - ETA: 0s - loss: 824.8052 - accuracy: 0.46 - ETA: 0s - loss: 789.7756 - accuracy: 0.45 - ETA: 0s - loss: 760.2574 - accuracy: 0.45 - ETA: 0s - loss: 737.9606 - accuracy: 0.44 - 3s 154ms/step - loss: 706.4904 - accuracy: 0.4517 - val_loss: 4.4478 - val_accuracy: 0.5402\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 51.0258 - accuracy: 0.562 - ETA: 2s - loss: 33.8765 - accuracy: 0.593 - ETA: 2s - loss: 80.8765 - accuracy: 0.562 - ETA: 2s - loss: 72.6969 - accuracy: 0.554 - ETA: 2s - loss: 71.1204 - accuracy: 0.562 - ETA: 2s - loss: 68.9831 - accuracy: 0.552 - ETA: 2s - loss: 66.7743 - accuracy: 0.567 - ETA: 2s - loss: 71.6206 - accuracy: 0.574 - ETA: 1s - loss: 70.2503 - accuracy: 0.572 - ETA: 1s - loss: 82.2384 - accuracy: 0.556 - ETA: 1s - loss: 157.0178 - accuracy: 0.54 - ETA: 1s - loss: 147.0414 - accuracy: 0.55 - ETA: 1s - loss: 166.2719 - accuracy: 0.56 - ETA: 1s - loss: 155.9374 - accuracy: 0.56 - ETA: 1s - loss: 149.6397 - accuracy: 0.56 - ETA: 0s - loss: 142.9013 - accuracy: 0.55 - ETA: 0s - loss: 137.1515 - accuracy: 0.55 - ETA: 0s - loss: 129.8738 - accuracy: 0.56 - ETA: 0s - loss: 123.4783 - accuracy: 0.55 - ETA: 0s - loss: 117.8488 - accuracy: 0.55 - ETA: 0s - loss: 112.5068 - accuracy: 0.55 - 4s 160ms/step - loss: 107.7617 - accuracy: 0.5540 - val_loss: 8.1778 - val_accuracy: 0.6071\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 8.6854 - accuracy: 0.59 - ETA: 2s - loss: 7.1110 - accuracy: 0.62 - ETA: 2s - loss: 7.2398 - accuracy: 0.61 - ETA: 2s - loss: 7.8768 - accuracy: 0.57 - ETA: 2s - loss: 7.7535 - accuracy: 0.55 - ETA: 2s - loss: 7.8796 - accuracy: 0.57 - ETA: 2s - loss: 7.1210 - accuracy: 0.55 - ETA: 1s - loss: 6.5821 - accuracy: 0.54 - ETA: 1s - loss: 6.7074 - accuracy: 0.54 - ETA: 1s - loss: 6.4214 - accuracy: 0.55 - ETA: 1s - loss: 6.0437 - accuracy: 0.55 - ETA: 1s - loss: 5.9469 - accuracy: 0.54 - ETA: 1s - loss: 5.7585 - accuracy: 0.54 - ETA: 1s - loss: 6.4109 - accuracy: 0.53 - ETA: 0s - loss: 6.1411 - accuracy: 0.52 - ETA: 0s - loss: 6.0020 - accuracy: 0.52 - ETA: 0s - loss: 5.7909 - accuracy: 0.52 - ETA: 0s - loss: 5.6532 - accuracy: 0.52 - ETA: 0s - loss: 5.5581 - accuracy: 0.51 - ETA: 0s - loss: 5.3805 - accuracy: 0.52 - ETA: 0s - loss: 5.2071 - accuracy: 0.52 - 3s 149ms/step - loss: 5.2333 - accuracy: 0.5270 - val_loss: 13.5739 - val_accuracy: 0.6830\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 2.8646 - accuracy: 0.56 - ETA: 2s - loss: 3.1827 - accuracy: 0.48 - ETA: 2s - loss: 3.0827 - accuracy: 0.51 - ETA: 2s - loss: 2.7932 - accuracy: 0.54 - ETA: 2s - loss: 2.9558 - accuracy: 0.54 - ETA: 2s - loss: 3.3311 - accuracy: 0.52 - ETA: 2s - loss: 3.8689 - accuracy: 0.51 - ETA: 1s - loss: 4.0923 - accuracy: 0.51 - ETA: 1s - loss: 4.2861 - accuracy: 0.51 - ETA: 1s - loss: 4.2876 - accuracy: 0.52 - ETA: 1s - loss: 4.5563 - accuracy: 0.51 - ETA: 1s - loss: 4.7222 - accuracy: 0.50 - ETA: 1s - loss: 4.7708 - accuracy: 0.50 - ETA: 1s - loss: 4.7856 - accuracy: 0.51 - ETA: 0s - loss: 4.7917 - accuracy: 0.52 - ETA: 0s - loss: 4.8935 - accuracy: 0.51 - ETA: 0s - loss: 5.0184 - accuracy: 0.51 - ETA: 0s - loss: 4.9838 - accuracy: 0.51 - ETA: 0s - loss: 4.9382 - accuracy: 0.51 - ETA: 0s - loss: 4.9511 - accuracy: 0.52 - ETA: 0s - loss: 4.9226 - accuracy: 0.51 - 3s 150ms/step - loss: 4.9827 - accuracy: 0.5170 - val_loss: 0.7969 - val_accuracy: 0.6027\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 4.3272 - accuracy: 0.59 - ETA: 2s - loss: 5.2573 - accuracy: 0.53 - ETA: 2s - loss: 4.9727 - accuracy: 0.53 - ETA: 2s - loss: 4.5807 - accuracy: 0.54 - ETA: 2s - loss: 4.5354 - accuracy: 0.55 - ETA: 2s - loss: 4.4635 - accuracy: 0.53 - ETA: 2s - loss: 4.5529 - accuracy: 0.53 - ETA: 1s - loss: 4.4421 - accuracy: 0.53 - ETA: 1s - loss: 4.4761 - accuracy: 0.54 - ETA: 1s - loss: 4.6829 - accuracy: 0.53 - ETA: 1s - loss: 4.6695 - accuracy: 0.53 - ETA: 1s - loss: 4.6760 - accuracy: 0.52 - ETA: 1s - loss: 4.6561 - accuracy: 0.53 - ETA: 1s - loss: 4.7529 - accuracy: 0.52 - ETA: 0s - loss: 4.5964 - accuracy: 0.54 - ETA: 0s - loss: 4.6841 - accuracy: 0.53 - ETA: 0s - loss: 4.5269 - accuracy: 0.54 - ETA: 0s - loss: 4.5395 - accuracy: 0.54 - ETA: 0s - loss: 4.5434 - accuracy: 0.54 - ETA: 0s - loss: 4.5302 - accuracy: 0.54 - ETA: 0s - loss: 4.4687 - accuracy: 0.54 - 3s 158ms/step - loss: 4.4208 - accuracy: 0.5426 - val_loss: 1.1129 - val_accuracy: 0.4554\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 3.4547 - accuracy: 0.43 - ETA: 2s - loss: 5.2855 - accuracy: 0.45 - ETA: 2s - loss: 4.4567 - accuracy: 0.46 - ETA: 2s - loss: 3.7888 - accuracy: 0.51 - ETA: 2s - loss: 3.8369 - accuracy: 0.51 - ETA: 2s - loss: 3.5070 - accuracy: 0.53 - ETA: 2s - loss: 3.5603 - accuracy: 0.50 - ETA: 2s - loss: 3.5790 - accuracy: 0.50 - ETA: 1s - loss: 3.7718 - accuracy: 0.49 - ETA: 1s - loss: 3.8971 - accuracy: 0.50 - ETA: 1s - loss: 3.7634 - accuracy: 0.50 - ETA: 1s - loss: 3.8063 - accuracy: 0.50 - ETA: 1s - loss: 3.8364 - accuracy: 0.49 - ETA: 1s - loss: 3.7017 - accuracy: 0.51 - ETA: 1s - loss: 3.7484 - accuracy: 0.51 - ETA: 0s - loss: 3.7659 - accuracy: 0.50 - ETA: 0s - loss: 3.7824 - accuracy: 0.50 - ETA: 0s - loss: 3.7300 - accuracy: 0.50 - ETA: 0s - loss: 3.6365 - accuracy: 0.51 - ETA: 0s - loss: 3.6479 - accuracy: 0.50 - ETA: 0s - loss: 3.6964 - accuracy: 0.50 - 4s 163ms/step - loss: 3.6554 - accuracy: 0.5114 - val_loss: 1.7819 - val_accuracy: 0.3571\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 2.7613 - accuracy: 0.59 - ETA: 2s - loss: 2.5498 - accuracy: 0.53 - ETA: 2s - loss: 3.3859 - accuracy: 0.51 - ETA: 2s - loss: 3.2044 - accuracy: 0.52 - ETA: 2s - loss: 3.0560 - accuracy: 0.52 - ETA: 2s - loss: 3.1708 - accuracy: 0.52 - ETA: 1s - loss: 3.3297 - accuracy: 0.51 - ETA: 1s - loss: 3.2946 - accuracy: 0.51 - ETA: 1s - loss: 3.1888 - accuracy: 0.52 - ETA: 1s - loss: 3.2735 - accuracy: 0.52 - ETA: 1s - loss: 3.1326 - accuracy: 0.53 - ETA: 1s - loss: 3.0033 - accuracy: 0.53 - ETA: 1s - loss: 2.9709 - accuracy: 0.54 - ETA: 0s - loss: 3.0538 - accuracy: 0.54 - ETA: 0s - loss: 2.9789 - accuracy: 0.53 - ETA: 0s - loss: 2.8704 - accuracy: 0.53 - ETA: 0s - loss: 2.9100 - accuracy: 0.53 - ETA: 0s - loss: 2.8742 - accuracy: 0.53 - ETA: 0s - loss: 2.8442 - accuracy: 0.53 - ETA: 0s - loss: 2.8175 - accuracy: 0.53 - ETA: 0s - loss: 2.8054 - accuracy: 0.53 - 3s 129ms/step - loss: 2.9390 - accuracy: 0.5256 - val_loss: 1.3001 - val_accuracy: 0.3616\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 3.6199 - accuracy: 0.43 - ETA: 2s - loss: 2.6834 - accuracy: 0.54 - ETA: 2s - loss: 2.5614 - accuracy: 0.57 - ETA: 2s - loss: 2.6213 - accuracy: 0.55 - ETA: 1s - loss: 2.4625 - accuracy: 0.58 - ETA: 1s - loss: 2.4939 - accuracy: 0.57 - ETA: 1s - loss: 2.5584 - accuracy: 0.57 - ETA: 1s - loss: 2.5163 - accuracy: 0.57 - ETA: 1s - loss: 2.4891 - accuracy: 0.57 - ETA: 1s - loss: 2.5162 - accuracy: 0.56 - ETA: 1s - loss: 2.5523 - accuracy: 0.56 - ETA: 1s - loss: 2.4794 - accuracy: 0.57 - ETA: 1s - loss: 2.6259 - accuracy: 0.55 - ETA: 0s - loss: 2.6790 - accuracy: 0.55 - ETA: 0s - loss: 2.5885 - accuracy: 0.55 - ETA: 0s - loss: 2.6069 - accuracy: 0.55 - ETA: 0s - loss: 2.6107 - accuracy: 0.54 - ETA: 0s - loss: 2.5336 - accuracy: 0.55 - ETA: 0s - loss: 2.5826 - accuracy: 0.55 - ETA: 0s - loss: 2.4994 - accuracy: 0.55 - ETA: 0s - loss: 2.4095 - accuracy: 0.56 - 3s 125ms/step - loss: 2.3982 - accuracy: 0.5639 - val_loss: 1.1763 - val_accuracy: 0.3884\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 2.4596 - accuracy: 0.46 - ETA: 2s - loss: 2.3243 - accuracy: 0.54 - ETA: 2s - loss: 2.7866 - accuracy: 0.48 - ETA: 2s - loss: 2.3998 - accuracy: 0.51 - ETA: 1s - loss: 2.2726 - accuracy: 0.55 - ETA: 1s - loss: 2.3505 - accuracy: 0.51 - ETA: 1s - loss: 2.2727 - accuracy: 0.52 - ETA: 1s - loss: 2.1433 - accuracy: 0.52 - ETA: 1s - loss: 2.0603 - accuracy: 0.53 - ETA: 1s - loss: 2.0247 - accuracy: 0.54 - ETA: 1s - loss: 2.0663 - accuracy: 0.54 - ETA: 1s - loss: 2.1588 - accuracy: 0.54 - ETA: 1s - loss: 2.0688 - accuracy: 0.54 - ETA: 0s - loss: 2.1159 - accuracy: 0.54 - ETA: 0s - loss: 2.1251 - accuracy: 0.53 - ETA: 0s - loss: 2.1446 - accuracy: 0.54 - ETA: 0s - loss: 2.2117 - accuracy: 0.54 - ETA: 0s - loss: 2.2947 - accuracy: 0.53 - ETA: 0s - loss: 2.3334 - accuracy: 0.53 - ETA: 0s - loss: 2.3098 - accuracy: 0.52 - ETA: 0s - loss: 2.2956 - accuracy: 0.52 - 3s 123ms/step - loss: 2.2583 - accuracy: 0.5298 - val_loss: 1.6663 - val_accuracy: 0.4062\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.8262 - accuracy: 0.56 - ETA: 2s - loss: 2.1336 - accuracy: 0.54 - ETA: 2s - loss: 1.9044 - accuracy: 0.54 - ETA: 1s - loss: 1.7099 - accuracy: 0.57 - ETA: 1s - loss: 1.7637 - accuracy: 0.56 - ETA: 1s - loss: 1.7832 - accuracy: 0.54 - ETA: 1s - loss: 1.8363 - accuracy: 0.53 - ETA: 1s - loss: 1.7601 - accuracy: 0.54 - ETA: 1s - loss: 1.8847 - accuracy: 0.53 - ETA: 1s - loss: 1.8808 - accuracy: 0.53 - ETA: 1s - loss: 1.8696 - accuracy: 0.53 - ETA: 1s - loss: 1.7959 - accuracy: 0.54 - ETA: 1s - loss: 1.8731 - accuracy: 0.53 - ETA: 0s - loss: 1.9210 - accuracy: 0.52 - ETA: 0s - loss: 1.9076 - accuracy: 0.53 - ETA: 0s - loss: 1.8243 - accuracy: 0.54 - ETA: 0s - loss: 1.8557 - accuracy: 0.54 - ETA: 0s - loss: 1.8454 - accuracy: 0.53 - ETA: 0s - loss: 1.8687 - accuracy: 0.53 - ETA: 0s - loss: 1.8390 - accuracy: 0.53 - ETA: 0s - loss: 1.8280 - accuracy: 0.52 - 3s 130ms/step - loss: 1.8539 - accuracy: 0.5270 - val_loss: 0.9595 - val_accuracy: 0.5223\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 2.2033 - accuracy: 0.43 - ETA: 2s - loss: 1.7015 - accuracy: 0.48 - ETA: 2s - loss: 1.8056 - accuracy: 0.46 - ETA: 2s - loss: 1.8173 - accuracy: 0.45 - ETA: 1s - loss: 1.7148 - accuracy: 0.47 - ETA: 1s - loss: 1.7317 - accuracy: 0.47 - ETA: 1s - loss: 1.6383 - accuracy: 0.50 - ETA: 1s - loss: 1.7036 - accuracy: 0.48 - ETA: 1s - loss: 1.6728 - accuracy: 0.47 - ETA: 1s - loss: 1.6807 - accuracy: 0.47 - ETA: 1s - loss: 1.6218 - accuracy: 0.49 - ETA: 1s - loss: 1.6378 - accuracy: 0.50 - ETA: 1s - loss: 1.6752 - accuracy: 0.50 - ETA: 0s - loss: 1.6908 - accuracy: 0.50 - ETA: 0s - loss: 1.7139 - accuracy: 0.49 - ETA: 0s - loss: 1.6801 - accuracy: 0.50 - ETA: 0s - loss: 1.6761 - accuracy: 0.50 - ETA: 0s - loss: 1.7893 - accuracy: 0.49 - ETA: 0s - loss: 1.7488 - accuracy: 0.50 - ETA: 0s - loss: 1.7267 - accuracy: 0.50 - ETA: 0s - loss: 1.7361 - accuracy: 0.49 - 3s 130ms/step - loss: 1.7001 - accuracy: 0.5014 - val_loss: 1.6801 - val_accuracy: 0.5312\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.7893 - accuracy: 0.46 - ETA: 2s - loss: 1.4844 - accuracy: 0.46 - ETA: 2s - loss: 1.5629 - accuracy: 0.43 - ETA: 1s - loss: 1.4779 - accuracy: 0.48 - ETA: 1s - loss: 1.5881 - accuracy: 0.46 - ETA: 1s - loss: 1.5165 - accuracy: 0.45 - ETA: 1s - loss: 1.4959 - accuracy: 0.45 - ETA: 1s - loss: 1.4424 - accuracy: 0.45 - ETA: 1s - loss: 1.3866 - accuracy: 0.47 - ETA: 1s - loss: 1.4481 - accuracy: 0.49 - ETA: 1s - loss: 1.4771 - accuracy: 0.50 - ETA: 1s - loss: 1.4456 - accuracy: 0.49 - ETA: 1s - loss: 1.4248 - accuracy: 0.50 - ETA: 0s - loss: 1.4072 - accuracy: 0.49 - ETA: 0s - loss: 1.4064 - accuracy: 0.50 - ETA: 0s - loss: 1.3681 - accuracy: 0.51 - ETA: 0s - loss: 1.4061 - accuracy: 0.51 - ETA: 0s - loss: 1.4111 - accuracy: 0.51 - ETA: 0s - loss: 1.4606 - accuracy: 0.51 - ETA: 0s - loss: 1.4567 - accuracy: 0.52 - ETA: 0s - loss: 1.4378 - accuracy: 0.51 - 3s 125ms/step - loss: 1.4479 - accuracy: 0.5128 - val_loss: 0.9444 - val_accuracy: 0.5938\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.1442 - accuracy: 0.56 - ETA: 2s - loss: 1.2046 - accuracy: 0.59 - ETA: 2s - loss: 1.5778 - accuracy: 0.57 - ETA: 2s - loss: 1.4331 - accuracy: 0.54 - ETA: 2s - loss: 1.4154 - accuracy: 0.53 - ETA: 1s - loss: 1.4612 - accuracy: 0.54 - ETA: 1s - loss: 1.6373 - accuracy: 0.54 - ETA: 1s - loss: 1.5806 - accuracy: 0.53 - ETA: 1s - loss: 1.5037 - accuracy: 0.53 - ETA: 1s - loss: 1.5387 - accuracy: 0.51 - ETA: 1s - loss: 1.5387 - accuracy: 0.51 - ETA: 1s - loss: 1.5107 - accuracy: 0.52 - ETA: 1s - loss: 1.5174 - accuracy: 0.51 - ETA: 0s - loss: 1.5528 - accuracy: 0.51 - ETA: 0s - loss: 1.5392 - accuracy: 0.51 - ETA: 0s - loss: 1.6643 - accuracy: 0.51 - ETA: 0s - loss: 1.6690 - accuracy: 0.51 - ETA: 0s - loss: 1.6394 - accuracy: 0.51 - ETA: 0s - loss: 1.6227 - accuracy: 0.51 - ETA: 0s - loss: 1.6567 - accuracy: 0.50 - ETA: 0s - loss: 1.7115 - accuracy: 0.50 - 3s 134ms/step - loss: 1.6750 - accuracy: 0.5028 - val_loss: 0.6095 - val_accuracy: 0.6384\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.4849 - accuracy: 0.59 - ETA: 2s - loss: 1.4609 - accuracy: 0.54 - ETA: 2s - loss: 1.4373 - accuracy: 0.56 - ETA: 2s - loss: 1.3286 - accuracy: 0.54 - ETA: 1s - loss: 1.5193 - accuracy: 0.56 - ETA: 1s - loss: 1.4881 - accuracy: 0.55 - ETA: 1s - loss: 1.4142 - accuracy: 0.56 - ETA: 1s - loss: 1.3453 - accuracy: 0.57 - ETA: 1s - loss: 1.5171 - accuracy: 0.54 - ETA: 1s - loss: 1.5887 - accuracy: 0.54 - ETA: 1s - loss: 1.5574 - accuracy: 0.53 - ETA: 1s - loss: 1.5182 - accuracy: 0.53 - ETA: 1s - loss: 1.4802 - accuracy: 0.54 - ETA: 0s - loss: 1.4603 - accuracy: 0.54 - ETA: 0s - loss: 1.4127 - accuracy: 0.54 - ETA: 0s - loss: 1.4396 - accuracy: 0.53 - ETA: 0s - loss: 1.6576 - accuracy: 0.54 - ETA: 0s - loss: 1.6286 - accuracy: 0.55 - ETA: 0s - loss: 1.5977 - accuracy: 0.55 - ETA: 0s - loss: 1.6093 - accuracy: 0.55 - ETA: 0s - loss: 1.6247 - accuracy: 0.54 - 3s 126ms/step - loss: 1.5952 - accuracy: 0.5384 - val_loss: 0.6112 - val_accuracy: 0.6295\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.1784 - accuracy: 0.53 - ETA: 2s - loss: 1.3207 - accuracy: 0.53 - ETA: 2s - loss: 1.2052 - accuracy: 0.54 - ETA: 1s - loss: 1.3728 - accuracy: 0.56 - ETA: 1s - loss: 1.3078 - accuracy: 0.56 - ETA: 1s - loss: 1.2288 - accuracy: 0.57 - ETA: 1s - loss: 1.2465 - accuracy: 0.55 - ETA: 1s - loss: 1.3319 - accuracy: 0.54 - ETA: 1s - loss: 1.3169 - accuracy: 0.55 - ETA: 2s - loss: 1.3491 - accuracy: 0.54 - ETA: 3s - loss: 1.3790 - accuracy: 0.53 - ETA: 2s - loss: 1.3549 - accuracy: 0.54 - ETA: 2s - loss: 1.3747 - accuracy: 0.53 - ETA: 2s - loss: 1.5785 - accuracy: 0.53 - ETA: 1s - loss: 1.5894 - accuracy: 0.54 - ETA: 1s - loss: 1.5994 - accuracy: 0.53 - ETA: 1s - loss: 1.5919 - accuracy: 0.52 - ETA: 0s - loss: 1.5491 - accuracy: 0.53 - ETA: 0s - loss: 1.5102 - accuracy: 0.53 - ETA: 0s - loss: 1.4974 - accuracy: 0.53 - ETA: 0s - loss: 1.5024 - accuracy: 0.52 - 5s 217ms/step - loss: 1.4776 - accuracy: 0.5284 - val_loss: 0.8398 - val_accuracy: 0.5804\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.2861 - accuracy: 0.53 - ETA: 2s - loss: 1.2787 - accuracy: 0.51 - ETA: 2s - loss: 1.2540 - accuracy: 0.52 - ETA: 1s - loss: 1.2812 - accuracy: 0.51 - ETA: 1s - loss: 1.3399 - accuracy: 0.48 - ETA: 1s - loss: 1.3325 - accuracy: 0.47 - ETA: 1s - loss: 1.3023 - accuracy: 0.48 - ETA: 1s - loss: 1.3320 - accuracy: 0.48 - ETA: 1s - loss: 1.3066 - accuracy: 0.49 - ETA: 1s - loss: 1.2744 - accuracy: 0.50 - ETA: 1s - loss: 1.2596 - accuracy: 0.51 - ETA: 1s - loss: 1.2521 - accuracy: 0.51 - ETA: 0s - loss: 1.5441 - accuracy: 0.51 - ETA: 0s - loss: 1.5095 - accuracy: 0.51 - ETA: 0s - loss: 1.4787 - accuracy: 0.50 - ETA: 0s - loss: 1.4499 - accuracy: 0.50 - ETA: 0s - loss: 1.4174 - accuracy: 0.51 - ETA: 0s - loss: 1.3766 - accuracy: 0.51 - ETA: 0s - loss: 1.3461 - accuracy: 0.52 - ETA: 0s - loss: 1.3297 - accuracy: 0.53 - ETA: 0s - loss: 1.3110 - accuracy: 0.52 - 5s 214ms/step - loss: 1.2881 - accuracy: 0.5312 - val_loss: 0.7629 - val_accuracy: 0.5938\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.0018 - accuracy: 0.62 - ETA: 2s - loss: 0.9595 - accuracy: 0.59 - ETA: 2s - loss: 0.8749 - accuracy: 0.61 - ETA: 2s - loss: 1.0550 - accuracy: 0.56 - ETA: 1s - loss: 1.0054 - accuracy: 0.56 - ETA: 1s - loss: 1.0098 - accuracy: 0.57 - ETA: 1s - loss: 0.9699 - accuracy: 0.58 - ETA: 1s - loss: 1.0645 - accuracy: 0.58 - ETA: 1s - loss: 1.0854 - accuracy: 0.57 - ETA: 1s - loss: 1.0690 - accuracy: 0.56 - ETA: 1s - loss: 1.0853 - accuracy: 0.57 - ETA: 1s - loss: 1.1209 - accuracy: 0.57 - ETA: 1s - loss: 1.1008 - accuracy: 0.57 - ETA: 0s - loss: 1.1169 - accuracy: 0.56 - ETA: 0s - loss: 1.1139 - accuracy: 0.56 - ETA: 0s - loss: 1.1471 - accuracy: 0.56 - ETA: 0s - loss: 1.1455 - accuracy: 0.55 - ETA: 0s - loss: 1.1752 - accuracy: 0.55 - ETA: 0s - loss: 1.1832 - accuracy: 0.54 - ETA: 0s - loss: 1.1911 - accuracy: 0.53 - ETA: 0s - loss: 1.1982 - accuracy: 0.53 - 3s 129ms/step - loss: 1.1947 - accuracy: 0.5355 - val_loss: 0.9131 - val_accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - ETA: 26s - loss: 0.8443 - accuracy: 0.625 - ETA: 23s - loss: 1.1820 - accuracy: 0.593 - ETA: 15s - loss: 1.3957 - accuracy: 0.562 - ETA: 11s - loss: 1.2436 - accuracy: 0.609 - ETA: 8s - loss: 1.2431 - accuracy: 0.581 - ETA: 7s - loss: 1.2224 - accuracy: 0.56 - ETA: 6s - loss: 1.1979 - accuracy: 0.55 - ETA: 5s - loss: 1.3169 - accuracy: 0.53 - ETA: 4s - loss: 1.2927 - accuracy: 0.53 - ETA: 3s - loss: 1.3674 - accuracy: 0.52 - ETA: 3s - loss: 1.3593 - accuracy: 0.51 - ETA: 2s - loss: 1.2934 - accuracy: 0.53 - ETA: 2s - loss: 1.2743 - accuracy: 0.52 - ETA: 2s - loss: 1.2444 - accuracy: 0.53 - ETA: 1s - loss: 1.2161 - accuracy: 0.54 - ETA: 1s - loss: 1.2058 - accuracy: 0.53 - ETA: 1s - loss: 1.2162 - accuracy: 0.54 - ETA: 0s - loss: 1.2644 - accuracy: 0.53 - ETA: 0s - loss: 1.2951 - accuracy: 0.52 - ETA: 0s - loss: 1.2957 - accuracy: 0.52 - ETA: 0s - loss: 1.2807 - accuracy: 0.53 - 5s 218ms/step - loss: 1.2672 - accuracy: 0.5327 - val_loss: 0.6546 - val_accuracy: 0.5893\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.1375 - accuracy: 0.56 - ETA: 2s - loss: 0.9775 - accuracy: 0.56 - ETA: 2s - loss: 1.0719 - accuracy: 0.54 - ETA: 2s - loss: 1.0863 - accuracy: 0.50 - ETA: 1s - loss: 1.0844 - accuracy: 0.49 - ETA: 1s - loss: 1.0501 - accuracy: 0.50 - ETA: 1s - loss: 1.0051 - accuracy: 0.50 - ETA: 1s - loss: 0.9974 - accuracy: 0.50 - ETA: 2s - loss: 1.0017 - accuracy: 0.50 - ETA: 3s - loss: 1.0414 - accuracy: 0.50 - ETA: 3s - loss: 1.1919 - accuracy: 0.50 - ETA: 2s - loss: 1.1649 - accuracy: 0.50 - ETA: 2s - loss: 1.2922 - accuracy: 0.49 - ETA: 2s - loss: 1.2665 - accuracy: 0.49 - ETA: 1s - loss: 1.2465 - accuracy: 0.50 - ETA: 1s - loss: 1.2416 - accuracy: 0.49 - ETA: 1s - loss: 1.2088 - accuracy: 0.50 - ETA: 0s - loss: 1.2112 - accuracy: 0.50 - ETA: 0s - loss: 1.1944 - accuracy: 0.50 - ETA: 0s - loss: 1.1868 - accuracy: 0.50 - ETA: 0s - loss: 1.1765 - accuracy: 0.50 - 5s 215ms/step - loss: 1.1538 - accuracy: 0.5099 - val_loss: 0.7157 - val_accuracy: 0.5982\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.0162 - accuracy: 0.56 - ETA: 2s - loss: 1.1371 - accuracy: 0.54 - ETA: 2s - loss: 1.1842 - accuracy: 0.56 - ETA: 1s - loss: 1.3302 - accuracy: 0.54 - ETA: 1s - loss: 1.3821 - accuracy: 0.54 - ETA: 1s - loss: 1.3250 - accuracy: 0.54 - ETA: 1s - loss: 1.3001 - accuracy: 0.54 - ETA: 1s - loss: 1.2407 - accuracy: 0.55 - ETA: 1s - loss: 1.1921 - accuracy: 0.56 - ETA: 1s - loss: 1.1329 - accuracy: 0.57 - ETA: 1s - loss: 1.1259 - accuracy: 0.57 - ETA: 1s - loss: 1.0990 - accuracy: 0.57 - ETA: 0s - loss: 1.0723 - accuracy: 0.57 - ETA: 0s - loss: 1.1601 - accuracy: 0.56 - ETA: 0s - loss: 1.1917 - accuracy: 0.55 - ETA: 0s - loss: 1.1814 - accuracy: 0.55 - ETA: 0s - loss: 1.1796 - accuracy: 0.54 - ETA: 0s - loss: 1.1724 - accuracy: 0.54 - ETA: 0s - loss: 1.1498 - accuracy: 0.54 - ETA: 0s - loss: 1.1375 - accuracy: 0.54 - ETA: 0s - loss: 1.1261 - accuracy: 0.55 - 5s 218ms/step - loss: 1.1177 - accuracy: 0.5625 - val_loss: 0.9028 - val_accuracy: 0.4911\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.8987 - accuracy: 0.59 - ETA: 2s - loss: 1.0984 - accuracy: 0.56 - ETA: 2s - loss: 1.2821 - accuracy: 0.55 - ETA: 1s - loss: 1.2414 - accuracy: 0.53 - ETA: 1s - loss: 1.2565 - accuracy: 0.51 - ETA: 1s - loss: 1.1983 - accuracy: 0.52 - ETA: 1s - loss: 1.2620 - accuracy: 0.51 - ETA: 1s - loss: 1.2135 - accuracy: 0.51 - ETA: 1s - loss: 1.1860 - accuracy: 0.51 - ETA: 1s - loss: 1.1894 - accuracy: 0.51 - ETA: 1s - loss: 1.1802 - accuracy: 0.52 - ETA: 1s - loss: 1.1558 - accuracy: 0.52 - ETA: 1s - loss: 1.1505 - accuracy: 0.52 - ETA: 0s - loss: 1.1185 - accuracy: 0.53 - ETA: 0s - loss: 1.1098 - accuracy: 0.53 - ETA: 0s - loss: 1.1213 - accuracy: 0.52 - ETA: 0s - loss: 1.1040 - accuracy: 0.53 - ETA: 0s - loss: 1.1335 - accuracy: 0.53 - ETA: 0s - loss: 1.2249 - accuracy: 0.53 - ETA: 0s - loss: 1.2100 - accuracy: 0.52 - ETA: 0s - loss: 1.2106 - accuracy: 0.52 - 3s 126ms/step - loss: 1.1985 - accuracy: 0.5256 - val_loss: 0.6540 - val_accuracy: 0.5893\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.9232 - accuracy: 0.68 - ETA: 2s - loss: 1.5334 - accuracy: 0.57 - ETA: 2s - loss: 1.5451 - accuracy: 0.50 - ETA: 2s - loss: 1.5079 - accuracy: 0.48 - ETA: 2s - loss: 1.4287 - accuracy: 0.48 - ETA: 2s - loss: 1.3417 - accuracy: 0.51 - ETA: 1s - loss: 1.2783 - accuracy: 0.52 - ETA: 1s - loss: 1.2433 - accuracy: 0.52 - ETA: 1s - loss: 1.2178 - accuracy: 0.52 - ETA: 1s - loss: 1.1954 - accuracy: 0.53 - ETA: 1s - loss: 1.2338 - accuracy: 0.51 - ETA: 1s - loss: 1.2377 - accuracy: 0.51 - ETA: 1s - loss: 1.2176 - accuracy: 0.51 - ETA: 1s - loss: 1.2064 - accuracy: 0.51 - ETA: 0s - loss: 1.1855 - accuracy: 0.52 - ETA: 0s - loss: 1.1544 - accuracy: 0.52 - ETA: 0s - loss: 1.1342 - accuracy: 0.53 - ETA: 0s - loss: 1.1358 - accuracy: 0.52 - ETA: 0s - loss: 1.1184 - accuracy: 0.52 - ETA: 0s - loss: 1.1221 - accuracy: 0.52 - ETA: 0s - loss: 1.1045 - accuracy: 0.52 - 5s 235ms/step - loss: 1.1059 - accuracy: 0.5256 - val_loss: 0.8016 - val_accuracy: 0.5357\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - ETA: 11s - loss: 2.5365 - accuracy: 0.656 - ETA: 7s - loss: 1.9441 - accuracy: 0.515 - ETA: 5s - loss: 1.6595 - accuracy: 0.52 - ETA: 4s - loss: 1.4862 - accuracy: 0.52 - ETA: 4s - loss: 1.4343 - accuracy: 0.51 - ETA: 3s - loss: 1.4790 - accuracy: 0.48 - ETA: 3s - loss: 1.4506 - accuracy: 0.48 - ETA: 2s - loss: 1.4094 - accuracy: 0.49 - ETA: 2s - loss: 1.3726 - accuracy: 0.49 - ETA: 2s - loss: 1.3096 - accuracy: 0.50 - ETA: 2s - loss: 1.2728 - accuracy: 0.50 - ETA: 1s - loss: 1.2661 - accuracy: 0.51 - ETA: 1s - loss: 1.2477 - accuracy: 0.51 - ETA: 1s - loss: 1.2325 - accuracy: 0.51 - ETA: 1s - loss: 1.2078 - accuracy: 0.51 - ETA: 1s - loss: 1.1946 - accuracy: 0.51 - ETA: 0s - loss: 1.1774 - accuracy: 0.51 - ETA: 0s - loss: 1.1673 - accuracy: 0.51 - ETA: 0s - loss: 1.1755 - accuracy: 0.51 - ETA: 0s - loss: 1.2058 - accuracy: 0.51 - ETA: 0s - loss: 1.2003 - accuracy: 0.51 - 6s 269ms/step - loss: 1.1910 - accuracy: 0.5071 - val_loss: 0.7403 - val_accuracy: 0.5000\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - ETA: 6s - loss: 0.7549 - accuracy: 0.56 - ETA: 4s - loss: 0.9621 - accuracy: 0.50 - ETA: 3s - loss: 1.1042 - accuracy: 0.47 - ETA: 3s - loss: 1.0238 - accuracy: 0.50 - ETA: 3s - loss: 1.0675 - accuracy: 0.47 - ETA: 2s - loss: 1.0845 - accuracy: 0.45 - ETA: 2s - loss: 1.0429 - accuracy: 0.48 - ETA: 2s - loss: 1.0661 - accuracy: 0.46 - ETA: 2s - loss: 1.0346 - accuracy: 0.47 - ETA: 2s - loss: 1.0164 - accuracy: 0.48 - ETA: 1s - loss: 1.0144 - accuracy: 0.49 - ETA: 1s - loss: 1.0196 - accuracy: 0.48 - ETA: 1s - loss: 1.0180 - accuracy: 0.48 - ETA: 1s - loss: 1.0628 - accuracy: 0.47 - ETA: 1s - loss: 1.0646 - accuracy: 0.48 - ETA: 0s - loss: 1.0516 - accuracy: 0.48 - ETA: 0s - loss: 1.0589 - accuracy: 0.48 - ETA: 0s - loss: 1.0522 - accuracy: 0.49 - ETA: 0s - loss: 1.0448 - accuracy: 0.49 - ETA: 0s - loss: 1.0379 - accuracy: 0.49 - ETA: 0s - loss: 1.0537 - accuracy: 0.49 - 5s 234ms/step - loss: 1.0464 - accuracy: 0.4972 - val_loss: 0.8472 - val_accuracy: 0.4955\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - ETA: 16s - loss: 0.9648 - accuracy: 0.531 - ETA: 10s - loss: 0.8695 - accuracy: 0.562 - ETA: 7s - loss: 0.8048 - accuracy: 0.593 - ETA: 6s - loss: 0.7720 - accuracy: 0.61 - ETA: 5s - loss: 0.9521 - accuracy: 0.59 - ETA: 4s - loss: 0.9241 - accuracy: 0.56 - ETA: 4s - loss: 0.9070 - accuracy: 0.54 - ETA: 3s - loss: 0.8941 - accuracy: 0.52 - ETA: 3s - loss: 0.9061 - accuracy: 0.52 - ETA: 3s - loss: 0.9037 - accuracy: 0.51 - ETA: 2s - loss: 0.9356 - accuracy: 0.52 - ETA: 2s - loss: 0.9354 - accuracy: 0.52 - ETA: 2s - loss: 0.9324 - accuracy: 0.51 - ETA: 1s - loss: 0.9474 - accuracy: 0.51 - ETA: 1s - loss: 0.9512 - accuracy: 0.51 - ETA: 1s - loss: 0.9437 - accuracy: 0.51 - ETA: 1s - loss: 0.9478 - accuracy: 0.51 - ETA: 0s - loss: 0.9420 - accuracy: 0.51 - ETA: 0s - loss: 0.9413 - accuracy: 0.51 - ETA: 0s - loss: 0.9329 - accuracy: 0.52 - ETA: 0s - loss: 0.9444 - accuracy: 0.51 - 7s 310ms/step - loss: 0.9534 - accuracy: 0.5185 - val_loss: 0.7856 - val_accuracy: 0.4688\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.8409 - accuracy: 0.59 - ETA: 2s - loss: 0.9940 - accuracy: 0.51 - ETA: 2s - loss: 0.9265 - accuracy: 0.52 - ETA: 2s - loss: 0.9465 - accuracy: 0.50 - ETA: 2s - loss: 0.9074 - accuracy: 0.53 - ETA: 2s - loss: 0.9055 - accuracy: 0.52 - ETA: 2s - loss: 0.9121 - accuracy: 0.51 - ETA: 2s - loss: 0.9271 - accuracy: 0.51 - ETA: 1s - loss: 0.9199 - accuracy: 0.51 - ETA: 1s - loss: 0.9164 - accuracy: 0.50 - ETA: 1s - loss: 0.9074 - accuracy: 0.51 - ETA: 1s - loss: 0.8913 - accuracy: 0.52 - ETA: 1s - loss: 0.9121 - accuracy: 0.52 - ETA: 1s - loss: 0.9154 - accuracy: 0.53 - ETA: 1s - loss: 0.9306 - accuracy: 0.53 - ETA: 0s - loss: 0.9415 - accuracy: 0.53 - ETA: 0s - loss: 0.9291 - accuracy: 0.54 - ETA: 0s - loss: 0.9546 - accuracy: 0.52 - ETA: 0s - loss: 0.9767 - accuracy: 0.52 - ETA: 0s - loss: 0.9827 - accuracy: 0.52 - ETA: 0s - loss: 0.9755 - accuracy: 0.52 - 5s 246ms/step - loss: 0.9833 - accuracy: 0.5185 - val_loss: 0.8351 - val_accuracy: 0.5580\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.0229 - accuracy: 0.46 - ETA: 2s - loss: 0.9727 - accuracy: 0.48 - ETA: 2s - loss: 0.8752 - accuracy: 0.52 - ETA: 2s - loss: 0.8812 - accuracy: 0.50 - ETA: 2s - loss: 0.8873 - accuracy: 0.48 - ETA: 2s - loss: 0.9051 - accuracy: 0.49 - ETA: 2s - loss: 0.8800 - accuracy: 0.50 - ETA: 1s - loss: 0.8983 - accuracy: 0.52 - ETA: 1s - loss: 0.8910 - accuracy: 0.54 - ETA: 1s - loss: 0.8522 - accuracy: 0.56 - ETA: 1s - loss: 0.8672 - accuracy: 0.55 - ETA: 1s - loss: 0.8449 - accuracy: 0.56 - ETA: 1s - loss: 0.8364 - accuracy: 0.57 - ETA: 1s - loss: 0.8275 - accuracy: 0.57 - ETA: 1s - loss: 0.8270 - accuracy: 0.56 - ETA: 0s - loss: 0.8473 - accuracy: 0.56 - ETA: 0s - loss: 0.8274 - accuracy: 0.57 - ETA: 0s - loss: 0.9088 - accuracy: 0.57 - ETA: 0s - loss: 0.9071 - accuracy: 0.57 - ETA: 0s - loss: 0.9096 - accuracy: 0.57 - ETA: 0s - loss: 0.9124 - accuracy: 0.56 - 5s 221ms/step - loss: 0.9064 - accuracy: 0.5625 - val_loss: 0.7613 - val_accuracy: 0.5580\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - ETA: 13s - loss: 0.9745 - accuracy: 0.593 - ETA: 7s - loss: 0.9442 - accuracy: 0.531 - ETA: 6s - loss: 1.0370 - accuracy: 0.50 - ETA: 4s - loss: 1.0708 - accuracy: 0.50 - ETA: 4s - loss: 1.0297 - accuracy: 0.52 - ETA: 3s - loss: 0.9826 - accuracy: 0.53 - ETA: 3s - loss: 1.0045 - accuracy: 0.51 - ETA: 2s - loss: 1.0382 - accuracy: 0.50 - ETA: 2s - loss: 1.0577 - accuracy: 0.49 - ETA: 2s - loss: 1.0466 - accuracy: 0.49 - ETA: 2s - loss: 1.0267 - accuracy: 0.49 - ETA: 1s - loss: 0.9976 - accuracy: 0.50 - ETA: 1s - loss: 0.9743 - accuracy: 0.50 - ETA: 1s - loss: 0.9868 - accuracy: 0.50 - ETA: 1s - loss: 1.0391 - accuracy: 0.50 - ETA: 1s - loss: 1.0312 - accuracy: 0.50 - ETA: 0s - loss: 1.0318 - accuracy: 0.51 - ETA: 0s - loss: 1.0675 - accuracy: 0.50 - ETA: 0s - loss: 1.0553 - accuracy: 0.51 - ETA: 0s - loss: 1.0365 - accuracy: 0.52 - ETA: 0s - loss: 1.0340 - accuracy: 0.51 - 4s 178ms/step - loss: 1.0155 - accuracy: 0.5241 - val_loss: 0.6814 - val_accuracy: 0.5491\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - ETA: 26s - loss: 1.1310 - accuracy: 0.593 - ETA: 23s - loss: 1.0459 - accuracy: 0.593 - ETA: 15s - loss: 0.9160 - accuracy: 0.614 - ETA: 11s - loss: 0.9807 - accuracy: 0.570 - ETA: 9s - loss: 0.9581 - accuracy: 0.556 - ETA: 7s - loss: 1.0185 - accuracy: 0.56 - ETA: 6s - loss: 0.9899 - accuracy: 0.56 - ETA: 5s - loss: 1.1368 - accuracy: 0.55 - ETA: 4s - loss: 1.0954 - accuracy: 0.56 - ETA: 4s - loss: 1.0800 - accuracy: 0.54 - ETA: 3s - loss: 1.0521 - accuracy: 0.55 - ETA: 3s - loss: 1.0344 - accuracy: 0.55 - ETA: 2s - loss: 1.0417 - accuracy: 0.55 - ETA: 2s - loss: 1.0048 - accuracy: 0.57 - ETA: 1s - loss: 0.9897 - accuracy: 0.57 - ETA: 1s - loss: 0.9888 - accuracy: 0.57 - ETA: 1s - loss: 0.9885 - accuracy: 0.57 - ETA: 1s - loss: 0.9797 - accuracy: 0.57 - ETA: 0s - loss: 0.9595 - accuracy: 0.58 - ETA: 0s - loss: 0.9649 - accuracy: 0.57 - ETA: 0s - loss: 0.9503 - accuracy: 0.58 - 5s 247ms/step - loss: 0.9437 - accuracy: 0.5824 - val_loss: 0.6660 - val_accuracy: 0.6518\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.7008 - accuracy: 0.71 - ETA: 2s - loss: 0.7631 - accuracy: 0.68 - ETA: 9s - loss: 0.7491 - accuracy: 0.65 - ETA: 11s - loss: 0.7825 - accuracy: 0.625 - ETA: 9s - loss: 0.8376 - accuracy: 0.593 - ETA: 7s - loss: 0.8454 - accuracy: 0.59 - ETA: 6s - loss: 0.9815 - accuracy: 0.58 - ETA: 5s - loss: 0.9986 - accuracy: 0.56 - ETA: 4s - loss: 0.9691 - accuracy: 0.56 - ETA: 4s - loss: 0.9798 - accuracy: 0.56 - ETA: 3s - loss: 0.9451 - accuracy: 0.57 - ETA: 3s - loss: 0.9778 - accuracy: 0.55 - ETA: 2s - loss: 0.9737 - accuracy: 0.55 - ETA: 2s - loss: 1.0280 - accuracy: 0.54 - ETA: 1s - loss: 1.0268 - accuracy: 0.54 - ETA: 1s - loss: 1.0273 - accuracy: 0.53 - ETA: 1s - loss: 1.0579 - accuracy: 0.53 - ETA: 1s - loss: 1.0476 - accuracy: 0.53 - ETA: 0s - loss: 1.0398 - accuracy: 0.53 - ETA: 0s - loss: 1.0232 - accuracy: 0.54 - ETA: 0s - loss: 1.0105 - accuracy: 0.54 - 5s 246ms/step - loss: 1.0213 - accuracy: 0.5426 - val_loss: 0.7493 - val_accuracy: 0.6027\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.6104 - accuracy: 0.68 - ETA: 3s - loss: 0.8769 - accuracy: 0.59 - ETA: 2s - loss: 0.8413 - accuracy: 0.59 - ETA: 4s - loss: 0.8840 - accuracy: 0.59 - ETA: 9s - loss: 0.8316 - accuracy: 0.61 - ETA: 7s - loss: 0.9111 - accuracy: 0.59 - ETA: 6s - loss: 0.8620 - accuracy: 0.61 - ETA: 5s - loss: 0.9089 - accuracy: 0.60 - ETA: 4s - loss: 0.8773 - accuracy: 0.60 - ETA: 4s - loss: 0.8569 - accuracy: 0.59 - ETA: 3s - loss: 0.9160 - accuracy: 0.57 - ETA: 3s - loss: 0.9018 - accuracy: 0.57 - ETA: 2s - loss: 0.9167 - accuracy: 0.56 - ETA: 2s - loss: 0.9024 - accuracy: 0.57 - ETA: 1s - loss: 0.8946 - accuracy: 0.57 - ETA: 1s - loss: 0.8833 - accuracy: 0.57 - ETA: 1s - loss: 0.8915 - accuracy: 0.56 - ETA: 1s - loss: 0.9030 - accuracy: 0.56 - ETA: 0s - loss: 0.9001 - accuracy: 0.57 - ETA: 0s - loss: 0.8907 - accuracy: 0.57 - ETA: 0s - loss: 0.8752 - accuracy: 0.57 - 6s 250ms/step - loss: 0.8664 - accuracy: 0.5866 - val_loss: 0.7190 - val_accuracy: 0.5625\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 2.1098 - accuracy: 0.37 - ETA: 3s - loss: 1.5279 - accuracy: 0.45 - ETA: 2s - loss: 1.2615 - accuracy: 0.47 - ETA: 2s - loss: 1.1200 - accuracy: 0.53 - ETA: 2s - loss: 1.0210 - accuracy: 0.55 - ETA: 5s - loss: 1.0281 - accuracy: 0.53 - ETA: 6s - loss: 0.9910 - accuracy: 0.53 - ETA: 5s - loss: 1.0208 - accuracy: 0.53 - ETA: 4s - loss: 1.0063 - accuracy: 0.53 - ETA: 4s - loss: 0.9690 - accuracy: 0.53 - ETA: 3s - loss: 0.9482 - accuracy: 0.53 - ETA: 3s - loss: 0.9501 - accuracy: 0.54 - ETA: 2s - loss: 0.9364 - accuracy: 0.54 - ETA: 2s - loss: 0.9252 - accuracy: 0.54 - ETA: 1s - loss: 0.9051 - accuracy: 0.55 - ETA: 1s - loss: 0.9159 - accuracy: 0.55 - ETA: 1s - loss: 0.9197 - accuracy: 0.55 - ETA: 1s - loss: 0.9066 - accuracy: 0.56 - ETA: 0s - loss: 0.8976 - accuracy: 0.56 - ETA: 0s - loss: 0.9043 - accuracy: 0.56 - ETA: 0s - loss: 0.9006 - accuracy: 0.56 - 5s 248ms/step - loss: 0.8914 - accuracy: 0.5668 - val_loss: 0.6908 - val_accuracy: 0.5312\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 1.1846 - accuracy: 0.43 - ETA: 2s - loss: 1.0722 - accuracy: 0.50 - ETA: 2s - loss: 1.0270 - accuracy: 0.53 - ETA: 2s - loss: 0.9804 - accuracy: 0.53 - ETA: 2s - loss: 0.9583 - accuracy: 0.53 - ETA: 2s - loss: 0.9503 - accuracy: 0.54 - ETA: 2s - loss: 0.9494 - accuracy: 0.54 - ETA: 5s - loss: 0.9230 - accuracy: 0.55 - ETA: 5s - loss: 0.9150 - accuracy: 0.54 - ETA: 4s - loss: 0.9486 - accuracy: 0.55 - ETA: 3s - loss: 0.9425 - accuracy: 0.55 - ETA: 3s - loss: 0.9353 - accuracy: 0.54 - ETA: 2s - loss: 0.9174 - accuracy: 0.54 - ETA: 2s - loss: 0.9049 - accuracy: 0.55 - ETA: 2s - loss: 0.9053 - accuracy: 0.55 - ETA: 1s - loss: 0.8947 - accuracy: 0.55 - ETA: 1s - loss: 0.8951 - accuracy: 0.55 - ETA: 1s - loss: 0.8923 - accuracy: 0.55 - ETA: 0s - loss: 0.8870 - accuracy: 0.55 - ETA: 0s - loss: 0.8768 - accuracy: 0.56 - ETA: 0s - loss: 0.8835 - accuracy: 0.55 - 6s 252ms/step - loss: 0.8790 - accuracy: 0.5682 - val_loss: 0.8136 - val_accuracy: 0.5580\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.1948 - accuracy: 0.53 - ETA: 2s - loss: 1.0866 - accuracy: 0.53 - ETA: 2s - loss: 1.1154 - accuracy: 0.54 - ETA: 1s - loss: 0.9945 - accuracy: 0.59 - ETA: 1s - loss: 0.9623 - accuracy: 0.60 - ETA: 1s - loss: 0.9309 - accuracy: 0.57 - ETA: 1s - loss: 0.9379 - accuracy: 0.55 - ETA: 1s - loss: 0.9501 - accuracy: 0.55 - ETA: 1s - loss: 0.9200 - accuracy: 0.56 - ETA: 1s - loss: 0.9059 - accuracy: 0.55 - ETA: 1s - loss: 0.9099 - accuracy: 0.55 - ETA: 1s - loss: 0.9254 - accuracy: 0.55 - ETA: 0s - loss: 0.9160 - accuracy: 0.55 - ETA: 0s - loss: 0.9211 - accuracy: 0.55 - ETA: 0s - loss: 0.9467 - accuracy: 0.55 - ETA: 0s - loss: 0.9439 - accuracy: 0.55 - ETA: 0s - loss: 0.9321 - accuracy: 0.54 - ETA: 0s - loss: 0.9367 - accuracy: 0.54 - ETA: 0s - loss: 0.9454 - accuracy: 0.53 - ETA: 0s - loss: 0.9879 - accuracy: 0.54 - ETA: 0s - loss: 0.9717 - accuracy: 0.54 - 5s 220ms/step - loss: 0.9727 - accuracy: 0.5455 - val_loss: 0.8008 - val_accuracy: 0.6027\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 1.1702 - accuracy: 0.46 - ETA: 2s - loss: 0.8810 - accuracy: 0.57 - ETA: 2s - loss: 0.8731 - accuracy: 0.58 - ETA: 2s - loss: 0.9116 - accuracy: 0.55 - ETA: 2s - loss: 0.8587 - accuracy: 0.56 - ETA: 2s - loss: 0.8263 - accuracy: 0.58 - ETA: 1s - loss: 0.8061 - accuracy: 0.59 - ETA: 1s - loss: 0.8015 - accuracy: 0.60 - ETA: 1s - loss: 0.8039 - accuracy: 0.60 - ETA: 1s - loss: 0.8130 - accuracy: 0.59 - ETA: 1s - loss: 0.8142 - accuracy: 0.59 - ETA: 1s - loss: 0.8232 - accuracy: 0.59 - ETA: 1s - loss: 0.8291 - accuracy: 0.58 - ETA: 1s - loss: 0.8305 - accuracy: 0.58 - ETA: 0s - loss: 0.8431 - accuracy: 0.57 - ETA: 0s - loss: 0.8379 - accuracy: 0.58 - ETA: 0s - loss: 0.8191 - accuracy: 0.59 - ETA: 0s - loss: 0.8352 - accuracy: 0.59 - ETA: 0s - loss: 0.8398 - accuracy: 0.59 - ETA: 0s - loss: 0.8422 - accuracy: 0.58 - ETA: 0s - loss: 0.8377 - accuracy: 0.59 - 4s 161ms/step - loss: 0.8342 - accuracy: 0.5895 - val_loss: 0.7851 - val_accuracy: 0.5982\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.6814 - accuracy: 0.65 - ETA: 3s - loss: 0.7425 - accuracy: 0.65 - ETA: 3s - loss: 0.7943 - accuracy: 0.59 - ETA: 2s - loss: 0.7806 - accuracy: 0.58 - ETA: 6s - loss: 0.8751 - accuracy: 0.57 - ETA: 8s - loss: 0.8886 - accuracy: 0.54 - ETA: 7s - loss: 0.9211 - accuracy: 0.55 - ETA: 6s - loss: 0.8925 - accuracy: 0.55 - ETA: 5s - loss: 0.8882 - accuracy: 0.56 - ETA: 4s - loss: 0.8822 - accuracy: 0.55 - ETA: 3s - loss: 0.8777 - accuracy: 0.57 - ETA: 3s - loss: 0.8690 - accuracy: 0.57 - ETA: 2s - loss: 0.8679 - accuracy: 0.57 - ETA: 2s - loss: 0.8803 - accuracy: 0.57 - ETA: 2s - loss: 0.8902 - accuracy: 0.56 - ETA: 1s - loss: 0.8848 - accuracy: 0.56 - ETA: 1s - loss: 0.9100 - accuracy: 0.55 - ETA: 1s - loss: 0.9288 - accuracy: 0.56 - ETA: 0s - loss: 0.9153 - accuracy: 0.56 - ETA: 0s - loss: 0.9077 - accuracy: 0.56 - ETA: 0s - loss: 0.9413 - accuracy: 0.56 - 6s 270ms/step - loss: 0.9263 - accuracy: 0.5682 - val_loss: 0.6357 - val_accuracy: 0.6071\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.8300 - accuracy: 0.50 - ETA: 3s - loss: 0.8629 - accuracy: 0.48 - ETA: 2s - loss: 0.8361 - accuracy: 0.51 - ETA: 9s - loss: 0.8620 - accuracy: 0.50 - ETA: 9s - loss: 0.8496 - accuracy: 0.55 - ETA: 8s - loss: 0.8560 - accuracy: 0.54 - ETA: 6s - loss: 0.8821 - accuracy: 0.54 - ETA: 5s - loss: 0.8927 - accuracy: 0.52 - ETA: 5s - loss: 0.9259 - accuracy: 0.51 - ETA: 4s - loss: 0.9376 - accuracy: 0.50 - ETA: 3s - loss: 0.9321 - accuracy: 0.50 - ETA: 3s - loss: 0.9305 - accuracy: 0.49 - ETA: 2s - loss: 0.9243 - accuracy: 0.49 - ETA: 2s - loss: 0.9615 - accuracy: 0.51 - ETA: 2s - loss: 0.9543 - accuracy: 0.51 - ETA: 1s - loss: 0.9564 - accuracy: 0.51 - ETA: 1s - loss: 0.9463 - accuracy: 0.51 - ETA: 1s - loss: 0.9343 - accuracy: 0.51 - ETA: 0s - loss: 0.9264 - accuracy: 0.51 - ETA: 0s - loss: 0.9101 - accuracy: 0.52 - ETA: 0s - loss: 0.9032 - accuracy: 0.52 - 6s 264ms/step - loss: 0.8977 - accuracy: 0.5241 - val_loss: 0.5691 - val_accuracy: 0.5804\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7677 - accuracy: 0.50 - ETA: 3s - loss: 0.8084 - accuracy: 0.50 - ETA: 8s - loss: 0.7986 - accuracy: 0.54 - ETA: 12s - loss: 0.7755 - accuracy: 0.531 - ETA: 9s - loss: 0.7979 - accuracy: 0.550 - ETA: 8s - loss: 0.7664 - accuracy: 0.57 - ETA: 6s - loss: 0.7496 - accuracy: 0.59 - ETA: 5s - loss: 0.7751 - accuracy: 0.58 - ETA: 5s - loss: 0.7773 - accuracy: 0.58 - ETA: 4s - loss: 0.7846 - accuracy: 0.58 - ETA: 3s - loss: 0.7793 - accuracy: 0.57 - ETA: 3s - loss: 0.7890 - accuracy: 0.57 - ETA: 2s - loss: 0.7814 - accuracy: 0.57 - ETA: 2s - loss: 0.7939 - accuracy: 0.57 - ETA: 2s - loss: 0.7850 - accuracy: 0.57 - ETA: 1s - loss: 0.7763 - accuracy: 0.58 - ETA: 1s - loss: 0.7880 - accuracy: 0.57 - ETA: 1s - loss: 0.7864 - accuracy: 0.58 - ETA: 0s - loss: 0.7896 - accuracy: 0.57 - ETA: 0s - loss: 0.7897 - accuracy: 0.57 - ETA: 0s - loss: 0.7969 - accuracy: 0.57 - 6s 263ms/step - loss: 0.8046 - accuracy: 0.5739 - val_loss: 0.6194 - val_accuracy: 0.6384\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.6490 - accuracy: 0.56 - ETA: 6s - loss: 0.7638 - accuracy: 0.53 - ETA: 12s - loss: 0.7110 - accuracy: 0.572 - ETA: 12s - loss: 0.7185 - accuracy: 0.601 - ETA: 9s - loss: 0.7595 - accuracy: 0.618 - ETA: 8s - loss: 0.8430 - accuracy: 0.59 - ETA: 6s - loss: 0.8610 - accuracy: 0.57 - ETA: 5s - loss: 0.8644 - accuracy: 0.57 - ETA: 5s - loss: 0.8425 - accuracy: 0.56 - ETA: 4s - loss: 0.8556 - accuracy: 0.56 - ETA: 3s - loss: 0.8591 - accuracy: 0.56 - ETA: 3s - loss: 0.8625 - accuracy: 0.55 - ETA: 2s - loss: 0.9055 - accuracy: 0.55 - ETA: 2s - loss: 0.9319 - accuracy: 0.53 - ETA: 2s - loss: 0.9584 - accuracy: 0.53 - ETA: 1s - loss: 0.9385 - accuracy: 0.53 - ETA: 1s - loss: 0.9280 - accuracy: 0.54 - ETA: 1s - loss: 0.9337 - accuracy: 0.53 - ETA: 0s - loss: 0.9109 - accuracy: 0.55 - ETA: 0s - loss: 0.9237 - accuracy: 0.55 - ETA: 0s - loss: 0.9254 - accuracy: 0.55 - 6s 266ms/step - loss: 0.9204 - accuracy: 0.5526 - val_loss: 0.7287 - val_accuracy: 0.6295\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - ETA: 9s - loss: 0.7839 - accuracy: 0.46 - ETA: 19s - loss: 0.8734 - accuracy: 0.484 - ETA: 16s - loss: 0.8160 - accuracy: 0.552 - ETA: 12s - loss: 0.8053 - accuracy: 0.570 - ETA: 9s - loss: 0.7961 - accuracy: 0.568 - ETA: 8s - loss: 0.7989 - accuracy: 0.57 - ETA: 6s - loss: 0.7951 - accuracy: 0.56 - ETA: 5s - loss: 0.7972 - accuracy: 0.57 - ETA: 5s - loss: 0.7790 - accuracy: 0.58 - ETA: 4s - loss: 0.7632 - accuracy: 0.60 - ETA: 3s - loss: 0.7655 - accuracy: 0.58 - ETA: 3s - loss: 0.7818 - accuracy: 0.59 - ETA: 2s - loss: 0.8154 - accuracy: 0.57 - ETA: 2s - loss: 0.8045 - accuracy: 0.57 - ETA: 2s - loss: 0.7973 - accuracy: 0.58 - ETA: 1s - loss: 0.7941 - accuracy: 0.58 - ETA: 1s - loss: 0.8045 - accuracy: 0.56 - ETA: 1s - loss: 0.8139 - accuracy: 0.56 - ETA: 0s - loss: 0.8166 - accuracy: 0.55 - ETA: 0s - loss: 0.8077 - accuracy: 0.55 - ETA: 0s - loss: 0.8089 - accuracy: 0.55 - 6s 259ms/step - loss: 0.8081 - accuracy: 0.5582 - val_loss: 0.7914 - val_accuracy: 0.6205\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - ETA: 8s - loss: 0.8155 - accuracy: 0.59 - ETA: 19s - loss: 0.6680 - accuracy: 0.687 - ETA: 15s - loss: 0.7197 - accuracy: 0.645 - ETA: 12s - loss: 0.7157 - accuracy: 0.609 - ETA: 9s - loss: 0.7532 - accuracy: 0.606 - ETA: 7s - loss: 0.7741 - accuracy: 0.60 - ETA: 6s - loss: 0.7918 - accuracy: 0.60 - ETA: 5s - loss: 0.8017 - accuracy: 0.58 - ETA: 5s - loss: 0.8047 - accuracy: 0.57 - ETA: 4s - loss: 0.8060 - accuracy: 0.57 - ETA: 3s - loss: 0.8192 - accuracy: 0.58 - ETA: 3s - loss: 0.8076 - accuracy: 0.58 - ETA: 2s - loss: 0.8076 - accuracy: 0.58 - ETA: 2s - loss: 0.8117 - accuracy: 0.58 - ETA: 2s - loss: 0.8010 - accuracy: 0.58 - ETA: 1s - loss: 0.7929 - accuracy: 0.58 - ETA: 1s - loss: 0.8018 - accuracy: 0.58 - ETA: 1s - loss: 0.8178 - accuracy: 0.59 - ETA: 0s - loss: 0.8098 - accuracy: 0.59 - ETA: 0s - loss: 0.8395 - accuracy: 0.58 - ETA: 0s - loss: 0.8255 - accuracy: 0.58 - 6s 260ms/step - loss: 0.8377 - accuracy: 0.5909 - val_loss: 0.6678 - val_accuracy: 0.6161\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - ETA: 11s - loss: 0.9155 - accuracy: 0.500 - ETA: 20s - loss: 0.8440 - accuracy: 0.562 - ETA: 15s - loss: 0.8413 - accuracy: 0.614 - ETA: 12s - loss: 0.8498 - accuracy: 0.570 - ETA: 9s - loss: 0.8103 - accuracy: 0.568 - ETA: 7s - loss: 0.7994 - accuracy: 0.57 - ETA: 6s - loss: 0.8146 - accuracy: 0.59 - ETA: 5s - loss: 0.8058 - accuracy: 0.59 - ETA: 5s - loss: 0.8104 - accuracy: 0.59 - ETA: 4s - loss: 0.8052 - accuracy: 0.59 - ETA: 3s - loss: 0.7824 - accuracy: 0.60 - ETA: 3s - loss: 0.7749 - accuracy: 0.60 - ETA: 2s - loss: 0.7686 - accuracy: 0.60 - ETA: 2s - loss: 0.7601 - accuracy: 0.60 - ETA: 2s - loss: 0.7749 - accuracy: 0.59 - ETA: 1s - loss: 0.7703 - accuracy: 0.59 - ETA: 1s - loss: 0.7737 - accuracy: 0.59 - ETA: 1s - loss: 0.7810 - accuracy: 0.59 - ETA: 0s - loss: 0.7819 - accuracy: 0.59 - ETA: 0s - loss: 0.7825 - accuracy: 0.58 - ETA: 0s - loss: 0.7891 - accuracy: 0.58 - 6s 260ms/step - loss: 0.7853 - accuracy: 0.5810 - val_loss: 0.6669 - val_accuracy: 0.5714\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - ETA: 18s - loss: 0.9600 - accuracy: 0.406 - ETA: 23s - loss: 0.9700 - accuracy: 0.468 - ETA: 16s - loss: 0.9459 - accuracy: 0.468 - ETA: 12s - loss: 0.9244 - accuracy: 0.523 - ETA: 10s - loss: 0.8589 - accuracy: 0.531 - ETA: 8s - loss: 0.8393 - accuracy: 0.526 - ETA: 7s - loss: 0.8700 - accuracy: 0.50 - ETA: 6s - loss: 0.8610 - accuracy: 0.50 - ETA: 5s - loss: 0.8564 - accuracy: 0.51 - ETA: 4s - loss: 0.8564 - accuracy: 0.52 - ETA: 4s - loss: 0.8461 - accuracy: 0.53 - ETA: 3s - loss: 0.8305 - accuracy: 0.53 - ETA: 3s - loss: 0.8301 - accuracy: 0.54 - ETA: 2s - loss: 0.8438 - accuracy: 0.53 - ETA: 2s - loss: 0.8457 - accuracy: 0.53 - ETA: 1s - loss: 0.8413 - accuracy: 0.52 - ETA: 1s - loss: 0.8286 - accuracy: 0.52 - ETA: 1s - loss: 0.8617 - accuracy: 0.52 - ETA: 0s - loss: 0.8511 - accuracy: 0.53 - ETA: 0s - loss: 0.8427 - accuracy: 0.53 - ETA: 0s - loss: 0.8305 - accuracy: 0.54 - 8s 380ms/step - loss: 0.8285 - accuracy: 0.5469 - val_loss: 0.7924 - val_accuracy: 0.5714\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - ETA: 2s - loss: 0.5781 - accuracy: 0.65 - ETA: 2s - loss: 0.7543 - accuracy: 0.65 - ETA: 3s - loss: 0.7610 - accuracy: 0.60 - ETA: 3s - loss: 0.8096 - accuracy: 0.56 - ETA: 2s - loss: 0.8262 - accuracy: 0.56 - ETA: 2s - loss: 0.8321 - accuracy: 0.57 - ETA: 2s - loss: 0.8735 - accuracy: 0.56 - ETA: 2s - loss: 0.8963 - accuracy: 0.55 - ETA: 2s - loss: 0.8985 - accuracy: 0.55 - ETA: 2s - loss: 0.9170 - accuracy: 0.54 - ETA: 1s - loss: 0.9126 - accuracy: 0.54 - ETA: 1s - loss: 0.9152 - accuracy: 0.54 - ETA: 1s - loss: 0.9050 - accuracy: 0.54 - ETA: 1s - loss: 0.9140 - accuracy: 0.53 - ETA: 1s - loss: 0.9012 - accuracy: 0.53 - ETA: 1s - loss: 0.8945 - accuracy: 0.54 - ETA: 0s - loss: 0.8922 - accuracy: 0.54 - ETA: 0s - loss: 0.8816 - accuracy: 0.55 - ETA: 0s - loss: 0.8789 - accuracy: 0.55 - ETA: 0s - loss: 0.8671 - accuracy: 0.55 - ETA: 0s - loss: 0.8704 - accuracy: 0.55 - 6s 280ms/step - loss: 0.8641 - accuracy: 0.5639 - val_loss: 0.8172 - val_accuracy: 0.5580\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.6232 - accuracy: 0.75 - ETA: 3s - loss: 0.6809 - accuracy: 0.65 - ETA: 3s - loss: 0.7056 - accuracy: 0.66 - ETA: 3s - loss: 0.7181 - accuracy: 0.65 - ETA: 3s - loss: 0.6951 - accuracy: 0.66 - ETA: 2s - loss: 0.6900 - accuracy: 0.66 - ETA: 2s - loss: 0.6891 - accuracy: 0.66 - ETA: 2s - loss: 0.7421 - accuracy: 0.62 - ETA: 2s - loss: 0.7320 - accuracy: 0.63 - ETA: 2s - loss: 0.7337 - accuracy: 0.63 - ETA: 1s - loss: 0.7403 - accuracy: 0.63 - ETA: 1s - loss: 0.7475 - accuracy: 0.61 - ETA: 1s - loss: 0.7461 - accuracy: 0.61 - ETA: 1s - loss: 0.7503 - accuracy: 0.60 - ETA: 1s - loss: 0.7673 - accuracy: 0.60 - ETA: 1s - loss: 0.7685 - accuracy: 0.59 - ETA: 1s - loss: 0.7578 - accuracy: 0.60 - ETA: 1s - loss: 0.7577 - accuracy: 0.60 - ETA: 0s - loss: 0.7604 - accuracy: 0.60 - ETA: 0s - loss: 0.7580 - accuracy: 0.60 - ETA: 0s - loss: 0.7597 - accuracy: 0.59 - 7s 296ms/step - loss: 0.7538 - accuracy: 0.6023 - val_loss: 0.6612 - val_accuracy: 0.6116\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.8288 - accuracy: 0.53 - ETA: 3s - loss: 0.7642 - accuracy: 0.59 - ETA: 3s - loss: 0.9132 - accuracy: 0.62 - ETA: 3s - loss: 0.9002 - accuracy: 0.60 - ETA: 3s - loss: 0.8857 - accuracy: 0.60 - ETA: 2s - loss: 0.8481 - accuracy: 0.61 - ETA: 2s - loss: 0.8406 - accuracy: 0.60 - ETA: 2s - loss: 0.8474 - accuracy: 0.59 - ETA: 2s - loss: 0.8547 - accuracy: 0.58 - ETA: 2s - loss: 0.8510 - accuracy: 0.57 - ETA: 4s - loss: 0.8337 - accuracy: 0.58 - ETA: 3s - loss: 0.8272 - accuracy: 0.58 - ETA: 3s - loss: 0.8140 - accuracy: 0.59 - ETA: 2s - loss: 0.8202 - accuracy: 0.58 - ETA: 2s - loss: 0.8189 - accuracy: 0.58 - ETA: 1s - loss: 0.8145 - accuracy: 0.58 - ETA: 1s - loss: 0.8654 - accuracy: 0.58 - ETA: 1s - loss: 0.8575 - accuracy: 0.58 - ETA: 0s - loss: 0.8812 - accuracy: 0.58 - ETA: 0s - loss: 0.8691 - accuracy: 0.58 - ETA: 0s - loss: 0.8712 - accuracy: 0.58 - 6s 295ms/step - loss: 0.8696 - accuracy: 0.5852 - val_loss: 0.7748 - val_accuracy: 0.6205\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - ETA: 4s - loss: 0.8582 - accuracy: 0.46 - ETA: 4s - loss: 0.8407 - accuracy: 0.46 - ETA: 4s - loss: 0.8314 - accuracy: 0.45 - ETA: 3s - loss: 0.8859 - accuracy: 0.46 - ETA: 3s - loss: 0.8399 - accuracy: 0.47 - ETA: 8s - loss: 0.8775 - accuracy: 0.50 - ETA: 7s - loss: 0.8988 - accuracy: 0.49 - ETA: 6s - loss: 0.8832 - accuracy: 0.49 - ETA: 5s - loss: 0.8687 - accuracy: 0.50 - ETA: 5s - loss: 0.8673 - accuracy: 0.51 - ETA: 4s - loss: 0.8535 - accuracy: 0.52 - ETA: 3s - loss: 0.8446 - accuracy: 0.53 - ETA: 3s - loss: 0.8522 - accuracy: 0.53 - ETA: 2s - loss: 0.8372 - accuracy: 0.54 - ETA: 2s - loss: 0.8456 - accuracy: 0.53 - ETA: 2s - loss: 0.8655 - accuracy: 0.52 - ETA: 1s - loss: 0.8707 - accuracy: 0.52 - ETA: 1s - loss: 0.8607 - accuracy: 0.52 - ETA: 0s - loss: 0.8707 - accuracy: 0.52 - ETA: 0s - loss: 0.8631 - accuracy: 0.53 - ETA: 0s - loss: 0.8688 - accuracy: 0.53 - 7s 300ms/step - loss: 0.8558 - accuracy: 0.5440 - val_loss: 0.7273 - val_accuracy: 0.6161\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - ETA: 4s - loss: 0.7151 - accuracy: 0.53 - ETA: 16s - loss: 0.8325 - accuracy: 0.515 - ETA: 16s - loss: 0.8012 - accuracy: 0.541 - ETA: 12s - loss: 0.7947 - accuracy: 0.585 - ETA: 9s - loss: 0.7953 - accuracy: 0.593 - ETA: 8s - loss: 0.7955 - accuracy: 0.56 - ETA: 6s - loss: 0.7803 - accuracy: 0.59 - ETA: 6s - loss: 0.7874 - accuracy: 0.58 - ETA: 5s - loss: 0.7901 - accuracy: 0.57 - ETA: 4s - loss: 0.7837 - accuracy: 0.57 - ETA: 3s - loss: 0.7756 - accuracy: 0.58 - ETA: 3s - loss: 0.7831 - accuracy: 0.57 - ETA: 2s - loss: 0.7865 - accuracy: 0.56 - ETA: 2s - loss: 0.7926 - accuracy: 0.56 - ETA: 2s - loss: 0.7841 - accuracy: 0.56 - ETA: 1s - loss: 0.7903 - accuracy: 0.56 - ETA: 1s - loss: 0.7806 - accuracy: 0.56 - ETA: 1s - loss: 0.7740 - accuracy: 0.56 - ETA: 0s - loss: 0.7729 - accuracy: 0.57 - ETA: 0s - loss: 0.7703 - accuracy: 0.56 - ETA: 0s - loss: 0.7763 - accuracy: 0.56 - 7s 326ms/step - loss: 0.7693 - accuracy: 0.5668 - val_loss: 0.5735 - val_accuracy: 0.5848\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - ETA: 21s - loss: 0.6545 - accuracy: 0.625 - ETA: 12s - loss: 0.6478 - accuracy: 0.640 - ETA: 8s - loss: 0.6705 - accuracy: 0.604 - ETA: 7s - loss: 0.7108 - accuracy: 0.57 - ETA: 6s - loss: 0.7111 - accuracy: 0.56 - ETA: 5s - loss: 0.7082 - accuracy: 0.58 - ETA: 4s - loss: 0.7310 - accuracy: 0.58 - ETA: 4s - loss: 0.7456 - accuracy: 0.58 - ETA: 3s - loss: 0.7311 - accuracy: 0.59 - ETA: 3s - loss: 0.7358 - accuracy: 0.59 - ETA: 3s - loss: 0.7442 - accuracy: 0.58 - ETA: 2s - loss: 0.7401 - accuracy: 0.59 - ETA: 2s - loss: 0.7479 - accuracy: 0.58 - ETA: 2s - loss: 0.7434 - accuracy: 0.59 - ETA: 1s - loss: 0.7460 - accuracy: 0.59 - ETA: 1s - loss: 0.7494 - accuracy: 0.59 - ETA: 1s - loss: 0.7572 - accuracy: 0.59 - ETA: 0s - loss: 0.7673 - accuracy: 0.59 - ETA: 0s - loss: 0.7645 - accuracy: 0.59 - ETA: 0s - loss: 0.7606 - accuracy: 0.58 - ETA: 0s - loss: 0.7594 - accuracy: 0.58 - 7s 324ms/step - loss: 0.7588 - accuracy: 0.5810 - val_loss: 0.6616 - val_accuracy: 0.6518\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.8294 - accuracy: 0.59 - ETA: 3s - loss: 1.9114 - accuracy: 0.64 - ETA: 3s - loss: 1.6394 - accuracy: 0.61 - ETA: 3s - loss: 1.5303 - accuracy: 0.55 - ETA: 2s - loss: 1.3495 - accuracy: 0.57 - ETA: 2s - loss: 1.2227 - accuracy: 0.60 - ETA: 2s - loss: 1.1689 - accuracy: 0.57 - ETA: 2s - loss: 1.1315 - accuracy: 0.55 - ETA: 2s - loss: 1.1050 - accuracy: 0.56 - ETA: 2s - loss: 1.0644 - accuracy: 0.56 - ETA: 1s - loss: 1.0235 - accuracy: 0.57 - ETA: 1s - loss: 0.9957 - accuracy: 0.57 - ETA: 1s - loss: 0.9782 - accuracy: 0.57 - ETA: 1s - loss: 0.9757 - accuracy: 0.56 - ETA: 1s - loss: 0.9531 - accuracy: 0.56 - ETA: 1s - loss: 0.9384 - accuracy: 0.57 - ETA: 0s - loss: 0.9209 - accuracy: 0.58 - ETA: 1s - loss: 0.9115 - accuracy: 0.57 - ETA: 0s - loss: 0.9064 - accuracy: 0.58 - ETA: 0s - loss: 0.8975 - accuracy: 0.58 - ETA: 0s - loss: 0.9087 - accuracy: 0.57 - 6s 275ms/step - loss: 0.9141 - accuracy: 0.5710 - val_loss: 0.6657 - val_accuracy: 0.6250\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.9001 - accuracy: 0.46 - ETA: 3s - loss: 0.7883 - accuracy: 0.60 - ETA: 3s - loss: 0.7612 - accuracy: 0.59 - ETA: 3s - loss: 0.7593 - accuracy: 0.57 - ETA: 2s - loss: 0.7746 - accuracy: 0.55 - ETA: 2s - loss: 0.7688 - accuracy: 0.55 - ETA: 2s - loss: 0.7580 - accuracy: 0.56 - ETA: 2s - loss: 0.7596 - accuracy: 0.57 - ETA: 2s - loss: 0.7493 - accuracy: 0.57 - ETA: 2s - loss: 0.7567 - accuracy: 0.56 - ETA: 1s - loss: 0.7559 - accuracy: 0.56 - ETA: 1s - loss: 0.7609 - accuracy: 0.55 - ETA: 1s - loss: 0.7602 - accuracy: 0.56 - ETA: 1s - loss: 0.7552 - accuracy: 0.56 - ETA: 1s - loss: 0.7509 - accuracy: 0.56 - ETA: 1s - loss: 0.7568 - accuracy: 0.56 - ETA: 1s - loss: 0.7560 - accuracy: 0.56 - ETA: 1s - loss: 0.7548 - accuracy: 0.56 - ETA: 0s - loss: 0.7528 - accuracy: 0.56 - ETA: 0s - loss: 0.7502 - accuracy: 0.56 - ETA: 0s - loss: 0.7441 - accuracy: 0.56 - 6s 269ms/step - loss: 0.7495 - accuracy: 0.5639 - val_loss: 0.7461 - val_accuracy: 0.5357\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7356 - accuracy: 0.62 - ETA: 3s - loss: 0.6594 - accuracy: 0.67 - ETA: 3s - loss: 0.7505 - accuracy: 0.63 - ETA: 3s - loss: 0.7792 - accuracy: 0.62 - ETA: 2s - loss: 0.8092 - accuracy: 0.58 - ETA: 2s - loss: 0.8092 - accuracy: 0.57 - ETA: 2s - loss: 0.8137 - accuracy: 0.57 - ETA: 2s - loss: 0.8188 - accuracy: 0.55 - ETA: 2s - loss: 0.8432 - accuracy: 0.56 - ETA: 2s - loss: 0.8288 - accuracy: 0.56 - ETA: 1s - loss: 0.8418 - accuracy: 0.55 - ETA: 1s - loss: 0.8537 - accuracy: 0.55 - ETA: 1s - loss: 0.8381 - accuracy: 0.55 - ETA: 1s - loss: 0.8328 - accuracy: 0.55 - ETA: 1s - loss: 0.8292 - accuracy: 0.55 - ETA: 1s - loss: 0.8222 - accuracy: 0.56 - ETA: 1s - loss: 0.8139 - accuracy: 0.56 - ETA: 1s - loss: 0.8113 - accuracy: 0.56 - ETA: 0s - loss: 0.8117 - accuracy: 0.56 - ETA: 0s - loss: 0.8727 - accuracy: 0.56 - ETA: 0s - loss: 0.8705 - accuracy: 0.56 - 6s 266ms/step - loss: 0.8577 - accuracy: 0.5668 - val_loss: 0.7501 - val_accuracy: 0.5491\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.8421 - accuracy: 0.43 - ETA: 3s - loss: 0.8335 - accuracy: 0.48 - ETA: 3s - loss: 0.8093 - accuracy: 0.50 - ETA: 2s - loss: 0.7961 - accuracy: 0.50 - ETA: 2s - loss: 0.7992 - accuracy: 0.51 - ETA: 2s - loss: 0.7718 - accuracy: 0.53 - ETA: 2s - loss: 0.7623 - accuracy: 0.54 - ETA: 2s - loss: 0.7644 - accuracy: 0.53 - ETA: 2s - loss: 0.7661 - accuracy: 0.53 - ETA: 2s - loss: 0.7509 - accuracy: 0.54 - ETA: 1s - loss: 0.7568 - accuracy: 0.53 - ETA: 1s - loss: 0.7749 - accuracy: 0.53 - ETA: 1s - loss: 0.7691 - accuracy: 0.54 - ETA: 1s - loss: 0.7714 - accuracy: 0.54 - ETA: 2s - loss: 0.7566 - accuracy: 0.55 - ETA: 1s - loss: 0.7608 - accuracy: 0.55 - ETA: 1s - loss: 0.7655 - accuracy: 0.55 - ETA: 1s - loss: 0.7648 - accuracy: 0.55 - ETA: 0s - loss: 0.7684 - accuracy: 0.55 - ETA: 0s - loss: 0.7691 - accuracy: 0.55 - ETA: 0s - loss: 0.7663 - accuracy: 0.55 - 6s 272ms/step - loss: 0.7711 - accuracy: 0.5511 - val_loss: 0.6972 - val_accuracy: 0.5982\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7264 - accuracy: 0.59 - ETA: 3s - loss: 0.7464 - accuracy: 0.54 - ETA: 3s - loss: 0.7828 - accuracy: 0.56 - ETA: 2s - loss: 0.7763 - accuracy: 0.57 - ETA: 2s - loss: 0.7793 - accuracy: 0.55 - ETA: 2s - loss: 0.7640 - accuracy: 0.55 - ETA: 2s - loss: 0.7399 - accuracy: 0.57 - ETA: 2s - loss: 0.7272 - accuracy: 0.57 - ETA: 2s - loss: 0.7290 - accuracy: 0.56 - ETA: 2s - loss: 0.7252 - accuracy: 0.57 - ETA: 1s - loss: 0.7351 - accuracy: 0.57 - ETA: 2s - loss: 0.7367 - accuracy: 0.58 - ETA: 2s - loss: 0.7455 - accuracy: 0.58 - ETA: 2s - loss: 0.7585 - accuracy: 0.58 - ETA: 2s - loss: 0.7685 - accuracy: 0.57 - ETA: 1s - loss: 0.7638 - accuracy: 0.57 - ETA: 1s - loss: 0.7598 - accuracy: 0.58 - ETA: 1s - loss: 0.7693 - accuracy: 0.56 - ETA: 0s - loss: 0.7683 - accuracy: 0.57 - ETA: 0s - loss: 0.7637 - accuracy: 0.58 - ETA: 0s - loss: 0.7575 - accuracy: 0.58 - 6s 268ms/step - loss: 0.7648 - accuracy: 0.5838 - val_loss: 0.7622 - val_accuracy: 0.6250\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 1.2149 - accuracy: 0.50 - ETA: 3s - loss: 1.1808 - accuracy: 0.46 - ETA: 3s - loss: 1.0243 - accuracy: 0.50 - ETA: 3s - loss: 0.9512 - accuracy: 0.53 - ETA: 2s - loss: 0.9096 - accuracy: 0.55 - ETA: 2s - loss: 0.8686 - accuracy: 0.57 - ETA: 2s - loss: 0.9374 - accuracy: 0.57 - ETA: 2s - loss: 0.9171 - accuracy: 0.57 - ETA: 2s - loss: 0.9083 - accuracy: 0.56 - ETA: 2s - loss: 0.8887 - accuracy: 0.56 - ETA: 3s - loss: 0.8764 - accuracy: 0.55 - ETA: 3s - loss: 0.8758 - accuracy: 0.54 - ETA: 2s - loss: 0.8866 - accuracy: 0.54 - ETA: 2s - loss: 0.8680 - accuracy: 0.55 - ETA: 2s - loss: 0.8591 - accuracy: 0.54 - ETA: 1s - loss: 0.8520 - accuracy: 0.55 - ETA: 1s - loss: 0.8487 - accuracy: 0.55 - ETA: 1s - loss: 0.8445 - accuracy: 0.55 - ETA: 0s - loss: 0.8394 - accuracy: 0.55 - ETA: 0s - loss: 0.8376 - accuracy: 0.55 - ETA: 0s - loss: 0.8273 - accuracy: 0.55 - 6s 271ms/step - loss: 0.8222 - accuracy: 0.5554 - val_loss: 0.6317 - val_accuracy: 0.6741\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7617 - accuracy: 0.62 - ETA: 3s - loss: 0.7668 - accuracy: 0.59 - ETA: 3s - loss: 0.7957 - accuracy: 0.59 - ETA: 3s - loss: 0.7527 - accuracy: 0.63 - ETA: 3s - loss: 0.7462 - accuracy: 0.62 - ETA: 2s - loss: 0.7404 - accuracy: 0.61 - ETA: 2s - loss: 0.7411 - accuracy: 0.60 - ETA: 2s - loss: 0.7505 - accuracy: 0.59 - ETA: 3s - loss: 0.7519 - accuracy: 0.58 - ETA: 4s - loss: 0.7587 - accuracy: 0.58 - ETA: 3s - loss: 0.8785 - accuracy: 0.56 - ETA: 3s - loss: 0.8692 - accuracy: 0.57 - ETA: 2s - loss: 0.8561 - accuracy: 0.57 - ETA: 2s - loss: 0.8451 - accuracy: 0.57 - ETA: 2s - loss: 0.8307 - accuracy: 0.58 - ETA: 1s - loss: 0.8171 - accuracy: 0.58 - ETA: 1s - loss: 0.8155 - accuracy: 0.58 - ETA: 1s - loss: 0.8197 - accuracy: 0.57 - ETA: 0s - loss: 0.8016 - accuracy: 0.58 - ETA: 0s - loss: 0.8083 - accuracy: 0.58 - ETA: 0s - loss: 0.8082 - accuracy: 0.58 - 6s 270ms/step - loss: 0.8189 - accuracy: 0.5753 - val_loss: 0.8314 - val_accuracy: 0.5357\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.8957 - accuracy: 0.53 - ETA: 3s - loss: 0.8404 - accuracy: 0.51 - ETA: 3s - loss: 0.7924 - accuracy: 0.52 - ETA: 3s - loss: 0.8242 - accuracy: 0.54 - ETA: 3s - loss: 0.8579 - accuracy: 0.55 - ETA: 2s - loss: 0.8672 - accuracy: 0.52 - ETA: 2s - loss: 0.8508 - accuracy: 0.52 - ETA: 5s - loss: 0.8281 - accuracy: 0.53 - ETA: 5s - loss: 0.8214 - accuracy: 0.53 - ETA: 4s - loss: 0.8167 - accuracy: 0.53 - ETA: 3s - loss: 0.8076 - accuracy: 0.53 - ETA: 3s - loss: 0.8008 - accuracy: 0.54 - ETA: 2s - loss: 0.8034 - accuracy: 0.54 - ETA: 2s - loss: 0.8030 - accuracy: 0.54 - ETA: 2s - loss: 0.8002 - accuracy: 0.54 - ETA: 1s - loss: 0.8094 - accuracy: 0.53 - ETA: 1s - loss: 0.8063 - accuracy: 0.53 - ETA: 1s - loss: 0.8003 - accuracy: 0.53 - ETA: 0s - loss: 0.7975 - accuracy: 0.53 - ETA: 0s - loss: 0.7923 - accuracy: 0.54 - ETA: 0s - loss: 0.7895 - accuracy: 0.54 - 6s 273ms/step - loss: 0.7850 - accuracy: 0.5469 - val_loss: 0.6674 - val_accuracy: 0.5804\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7757 - accuracy: 0.59 - ETA: 3s - loss: 0.7648 - accuracy: 0.57 - ETA: 3s - loss: 0.8422 - accuracy: 0.56 - ETA: 3s - loss: 0.8034 - accuracy: 0.57 - ETA: 3s - loss: 0.8125 - accuracy: 0.56 - ETA: 5s - loss: 0.7898 - accuracy: 0.58 - ETA: 6s - loss: 0.7971 - accuracy: 0.58 - ETA: 6s - loss: 0.7905 - accuracy: 0.57 - ETA: 5s - loss: 0.7990 - accuracy: 0.56 - ETA: 4s - loss: 0.7769 - accuracy: 0.58 - ETA: 4s - loss: 0.7661 - accuracy: 0.59 - ETA: 3s - loss: 0.7546 - accuracy: 0.59 - ETA: 3s - loss: 0.7567 - accuracy: 0.59 - ETA: 2s - loss: 0.7646 - accuracy: 0.58 - ETA: 2s - loss: 0.7607 - accuracy: 0.58 - ETA: 1s - loss: 0.7571 - accuracy: 0.58 - ETA: 1s - loss: 0.7774 - accuracy: 0.57 - ETA: 1s - loss: 0.7821 - accuracy: 0.57 - ETA: 0s - loss: 0.7830 - accuracy: 0.57 - ETA: 0s - loss: 0.7762 - accuracy: 0.57 - ETA: 0s - loss: 0.7942 - accuracy: 0.58 - 6s 277ms/step - loss: 0.7942 - accuracy: 0.5795 - val_loss: 0.6161 - val_accuracy: 0.6161\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7880 - accuracy: 0.59 - ETA: 3s - loss: 0.8119 - accuracy: 0.53 - ETA: 3s - loss: 0.7859 - accuracy: 0.55 - ETA: 9s - loss: 0.8202 - accuracy: 0.50 - ETA: 9s - loss: 0.8120 - accuracy: 0.51 - ETA: 8s - loss: 0.7797 - accuracy: 0.53 - ETA: 7s - loss: 0.7612 - accuracy: 0.55 - ETA: 6s - loss: 0.7490 - accuracy: 0.57 - ETA: 5s - loss: 0.7271 - accuracy: 0.59 - ETA: 4s - loss: 0.7282 - accuracy: 0.59 - ETA: 3s - loss: 0.7247 - accuracy: 0.59 - ETA: 3s - loss: 0.7137 - accuracy: 0.60 - ETA: 2s - loss: 0.7404 - accuracy: 0.59 - ETA: 2s - loss: 0.7441 - accuracy: 0.59 - ETA: 2s - loss: 0.7940 - accuracy: 0.59 - ETA: 1s - loss: 0.8043 - accuracy: 0.59 - ETA: 1s - loss: 0.7993 - accuracy: 0.58 - ETA: 1s - loss: 0.7908 - accuracy: 0.59 - ETA: 0s - loss: 0.7892 - accuracy: 0.58 - ETA: 0s - loss: 0.7902 - accuracy: 0.57 - ETA: 0s - loss: 0.7896 - accuracy: 0.57 - 6s 272ms/step - loss: 0.7814 - accuracy: 0.5795 - val_loss: 0.6692 - val_accuracy: 0.6384\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - ETA: 4s - loss: 0.6542 - accuracy: 0.56 - ETA: 14s - loss: 0.8268 - accuracy: 0.500 - ETA: 16s - loss: 0.8516 - accuracy: 0.489 - ETA: 12s - loss: 0.7775 - accuracy: 0.546 - ETA: 9s - loss: 0.7831 - accuracy: 0.543 - ETA: 8s - loss: 0.7744 - accuracy: 0.57 - ETA: 6s - loss: 0.7416 - accuracy: 0.59 - ETA: 6s - loss: 0.7483 - accuracy: 0.60 - ETA: 5s - loss: 0.7673 - accuracy: 0.59 - ETA: 4s - loss: 0.7563 - accuracy: 0.59 - ETA: 3s - loss: 0.7516 - accuracy: 0.59 - ETA: 3s - loss: 0.7773 - accuracy: 0.58 - ETA: 2s - loss: 0.7710 - accuracy: 0.58 - ETA: 2s - loss: 0.7650 - accuracy: 0.58 - ETA: 2s - loss: 0.7742 - accuracy: 0.57 - ETA: 1s - loss: 0.7782 - accuracy: 0.57 - ETA: 1s - loss: 0.7887 - accuracy: 0.56 - ETA: 1s - loss: 0.8041 - accuracy: 0.56 - ETA: 0s - loss: 0.7933 - accuracy: 0.57 - ETA: 0s - loss: 0.7938 - accuracy: 0.56 - ETA: 0s - loss: 0.7948 - accuracy: 0.56 - 8s 386ms/step - loss: 0.7955 - accuracy: 0.5639 - val_loss: 0.6407 - val_accuracy: 0.5446\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7132 - accuracy: 0.62 - ETA: 3s - loss: 0.7372 - accuracy: 0.57 - ETA: 3s - loss: 0.7647 - accuracy: 0.53 - ETA: 3s - loss: 0.7429 - accuracy: 0.53 - ETA: 3s - loss: 0.7544 - accuracy: 0.52 - ETA: 2s - loss: 0.7538 - accuracy: 0.53 - ETA: 2s - loss: 0.7530 - accuracy: 0.52 - ETA: 2s - loss: 0.7344 - accuracy: 0.55 - ETA: 2s - loss: 0.7160 - accuracy: 0.56 - ETA: 2s - loss: 0.7139 - accuracy: 0.57 - ETA: 2s - loss: 0.7261 - accuracy: 0.59 - ETA: 1s - loss: 0.7217 - accuracy: 0.59 - ETA: 1s - loss: 0.7184 - accuracy: 0.59 - ETA: 1s - loss: 0.7193 - accuracy: 0.58 - ETA: 1s - loss: 0.7216 - accuracy: 0.59 - ETA: 1s - loss: 0.7169 - accuracy: 0.59 - ETA: 1s - loss: 0.7384 - accuracy: 0.58 - ETA: 1s - loss: 0.7341 - accuracy: 0.59 - ETA: 0s - loss: 0.7326 - accuracy: 0.58 - ETA: 0s - loss: 0.7307 - accuracy: 0.58 - ETA: 0s - loss: 0.7282 - accuracy: 0.59 - 7s 299ms/step - loss: 0.7280 - accuracy: 0.5866 - val_loss: 0.7898 - val_accuracy: 0.5938\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - ETA: 4s - loss: 0.6518 - accuracy: 0.59 - ETA: 3s - loss: 0.6700 - accuracy: 0.62 - ETA: 3s - loss: 0.6679 - accuracy: 0.64 - ETA: 3s - loss: 0.7009 - accuracy: 0.64 - ETA: 3s - loss: 0.6930 - accuracy: 0.65 - ETA: 3s - loss: 0.6775 - accuracy: 0.66 - ETA: 2s - loss: 0.6759 - accuracy: 0.64 - ETA: 2s - loss: 0.6839 - accuracy: 0.62 - ETA: 2s - loss: 0.6884 - accuracy: 0.62 - ETA: 3s - loss: 0.7061 - accuracy: 0.61 - ETA: 6s - loss: 0.7046 - accuracy: 0.61 - ETA: 5s - loss: 0.6999 - accuracy: 0.61 - ETA: 4s - loss: 0.7089 - accuracy: 0.61 - ETA: 3s - loss: 0.7171 - accuracy: 0.60 - ETA: 3s - loss: 0.7198 - accuracy: 0.59 - ETA: 2s - loss: 0.7382 - accuracy: 0.59 - ETA: 2s - loss: 0.7382 - accuracy: 0.59 - ETA: 1s - loss: 0.7465 - accuracy: 0.59 - ETA: 1s - loss: 0.7573 - accuracy: 0.58 - ETA: 0s - loss: 0.7596 - accuracy: 0.58 - ETA: 0s - loss: 0.7705 - accuracy: 0.58 - 11s 479ms/step - loss: 0.7667 - accuracy: 0.5852 - val_loss: 0.6715 - val_accuracy: 0.6786\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - ETA: 4s - loss: 0.8818 - accuracy: 0.40 - ETA: 3s - loss: 0.8925 - accuracy: 0.43 - ETA: 3s - loss: 0.8859 - accuracy: 0.48 - ETA: 3s - loss: 0.8512 - accuracy: 0.51 - ETA: 3s - loss: 0.8607 - accuracy: 0.50 - ETA: 3s - loss: 0.8489 - accuracy: 0.52 - ETA: 2s - loss: 0.8264 - accuracy: 0.54 - ETA: 2s - loss: 0.8573 - accuracy: 0.54 - ETA: 2s - loss: 0.8585 - accuracy: 0.52 - ETA: 2s - loss: 0.8503 - accuracy: 0.53 - ETA: 2s - loss: 0.8331 - accuracy: 0.54 - ETA: 1s - loss: 0.8356 - accuracy: 0.53 - ETA: 1s - loss: 0.8350 - accuracy: 0.54 - ETA: 1s - loss: 0.8242 - accuracy: 0.55 - ETA: 1s - loss: 0.8199 - accuracy: 0.55 - ETA: 1s - loss: 0.8088 - accuracy: 0.55 - ETA: 0s - loss: 0.8006 - accuracy: 0.55 - ETA: 0s - loss: 0.7931 - accuracy: 0.56 - ETA: 0s - loss: 0.8014 - accuracy: 0.56 - ETA: 0s - loss: 0.8034 - accuracy: 0.56 - ETA: 0s - loss: 0.7946 - accuracy: 0.56 - 6s 277ms/step - loss: 0.7891 - accuracy: 0.5696 - val_loss: 0.6295 - val_accuracy: 0.5446\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.6081 - accuracy: 0.62 - ETA: 3s - loss: 0.6391 - accuracy: 0.64 - ETA: 3s - loss: 0.6709 - accuracy: 0.61 - ETA: 3s - loss: 0.8355 - accuracy: 0.57 - ETA: 2s - loss: 0.7989 - accuracy: 0.58 - ETA: 2s - loss: 0.8164 - accuracy: 0.59 - ETA: 2s - loss: 0.7943 - accuracy: 0.61 - ETA: 2s - loss: 0.7634 - accuracy: 0.62 - ETA: 2s - loss: 0.7701 - accuracy: 0.62 - ETA: 2s - loss: 0.7739 - accuracy: 0.61 - ETA: 1s - loss: 0.7802 - accuracy: 0.61 - ETA: 1s - loss: 0.7737 - accuracy: 0.60 - ETA: 1s - loss: 0.7712 - accuracy: 0.60 - ETA: 1s - loss: 0.7741 - accuracy: 0.61 - ETA: 1s - loss: 0.7662 - accuracy: 0.62 - ETA: 0s - loss: 0.7704 - accuracy: 0.61 - ETA: 0s - loss: 0.7564 - accuracy: 0.62 - ETA: 0s - loss: 0.7535 - accuracy: 0.62 - ETA: 0s - loss: 0.7526 - accuracy: 0.62 - ETA: 0s - loss: 0.7516 - accuracy: 0.62 - ETA: 0s - loss: 0.7516 - accuracy: 0.61 - 6s 277ms/step - loss: 0.7492 - accuracy: 0.6151 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.8294 - accuracy: 0.53 - ETA: 3s - loss: 0.7108 - accuracy: 0.62 - ETA: 3s - loss: 0.7011 - accuracy: 0.61 - ETA: 3s - loss: 0.6999 - accuracy: 0.62 - ETA: 3s - loss: 0.7137 - accuracy: 0.60 - ETA: 2s - loss: 0.6993 - accuracy: 0.61 - ETA: 2s - loss: 0.7036 - accuracy: 0.60 - ETA: 2s - loss: 0.6923 - accuracy: 0.61 - ETA: 2s - loss: 0.7030 - accuracy: 0.61 - ETA: 2s - loss: 0.7042 - accuracy: 0.61 - ETA: 1s - loss: 0.7072 - accuracy: 0.61 - ETA: 1s - loss: 0.7094 - accuracy: 0.60 - ETA: 1s - loss: 0.7090 - accuracy: 0.60 - ETA: 1s - loss: 0.7157 - accuracy: 0.60 - ETA: 1s - loss: 0.7138 - accuracy: 0.61 - ETA: 1s - loss: 0.7098 - accuracy: 0.60 - ETA: 1s - loss: 0.7051 - accuracy: 0.60 - ETA: 1s - loss: 0.7163 - accuracy: 0.60 - ETA: 0s - loss: 0.7132 - accuracy: 0.60 - ETA: 0s - loss: 0.7103 - accuracy: 0.60 - ETA: 0s - loss: 0.7037 - accuracy: 0.61 - 6s 264ms/step - loss: 0.6999 - accuracy: 0.6122 - val_loss: 0.6568 - val_accuracy: 0.6027\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.6913 - accuracy: 0.59 - ETA: 2s - loss: 0.6685 - accuracy: 0.64 - ETA: 3s - loss: 0.6717 - accuracy: 0.64 - ETA: 2s - loss: 0.6969 - accuracy: 0.62 - ETA: 2s - loss: 0.7136 - accuracy: 0.61 - ETA: 2s - loss: 0.7168 - accuracy: 0.59 - ETA: 2s - loss: 0.7579 - accuracy: 0.59 - ETA: 2s - loss: 0.7528 - accuracy: 0.60 - ETA: 2s - loss: 0.7424 - accuracy: 0.60 - ETA: 2s - loss: 0.7376 - accuracy: 0.60 - ETA: 1s - loss: 0.7410 - accuracy: 0.59 - ETA: 1s - loss: 0.7543 - accuracy: 0.57 - ETA: 1s - loss: 0.7501 - accuracy: 0.57 - ETA: 1s - loss: 0.7533 - accuracy: 0.57 - ETA: 2s - loss: 0.7476 - accuracy: 0.57 - ETA: 1s - loss: 0.7399 - accuracy: 0.58 - ETA: 1s - loss: 0.7492 - accuracy: 0.57 - ETA: 1s - loss: 0.7507 - accuracy: 0.57 - ETA: 0s - loss: 0.7536 - accuracy: 0.58 - ETA: 0s - loss: 0.7556 - accuracy: 0.57 - ETA: 0s - loss: 0.7623 - accuracy: 0.57 - 6s 271ms/step - loss: 0.7528 - accuracy: 0.5838 - val_loss: 0.7179 - val_accuracy: 0.6071\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7158 - accuracy: 0.46 - ETA: 3s - loss: 0.6605 - accuracy: 0.57 - ETA: 3s - loss: 0.6751 - accuracy: 0.58 - ETA: 3s - loss: 0.7500 - accuracy: 0.54 - ETA: 2s - loss: 0.7378 - accuracy: 0.54 - ETA: 2s - loss: 0.7623 - accuracy: 0.54 - ETA: 2s - loss: 0.7253 - accuracy: 0.56 - ETA: 2s - loss: 0.7260 - accuracy: 0.57 - ETA: 2s - loss: 0.7179 - accuracy: 0.57 - ETA: 2s - loss: 0.7116 - accuracy: 0.57 - ETA: 1s - loss: 0.7165 - accuracy: 0.57 - ETA: 1s - loss: 0.7143 - accuracy: 0.57 - ETA: 2s - loss: 0.7159 - accuracy: 0.58 - ETA: 2s - loss: 0.7158 - accuracy: 0.58 - ETA: 2s - loss: 0.7102 - accuracy: 0.59 - ETA: 1s - loss: 0.7016 - accuracy: 0.59 - ETA: 1s - loss: 0.7004 - accuracy: 0.59 - ETA: 1s - loss: 0.6999 - accuracy: 0.59 - ETA: 0s - loss: 0.6952 - accuracy: 0.60 - ETA: 0s - loss: 0.7009 - accuracy: 0.60 - ETA: 0s - loss: 0.6977 - accuracy: 0.59 - 6s 267ms/step - loss: 0.7064 - accuracy: 0.5966 - val_loss: 0.7629 - val_accuracy: 0.6027\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7526 - accuracy: 0.56 - ETA: 3s - loss: 0.6631 - accuracy: 0.62 - ETA: 3s - loss: 0.6766 - accuracy: 0.62 - ETA: 3s - loss: 0.6952 - accuracy: 0.60 - ETA: 2s - loss: 0.7549 - accuracy: 0.60 - ETA: 2s - loss: 0.7473 - accuracy: 0.60 - ETA: 2s - loss: 0.7412 - accuracy: 0.60 - ETA: 2s - loss: 0.7530 - accuracy: 0.58 - ETA: 2s - loss: 0.7403 - accuracy: 0.59 - ETA: 2s - loss: 0.7242 - accuracy: 0.59 - ETA: 2s - loss: 0.7232 - accuracy: 0.59 - ETA: 3s - loss: 0.7344 - accuracy: 0.57 - ETA: 2s - loss: 0.7331 - accuracy: 0.56 - ETA: 2s - loss: 0.7317 - accuracy: 0.56 - ETA: 2s - loss: 0.7328 - accuracy: 0.56 - ETA: 1s - loss: 0.7327 - accuracy: 0.56 - ETA: 1s - loss: 0.7243 - accuracy: 0.57 - ETA: 1s - loss: 0.7276 - accuracy: 0.57 - ETA: 0s - loss: 0.7249 - accuracy: 0.57 - ETA: 0s - loss: 0.7221 - accuracy: 0.57 - ETA: 0s - loss: 0.7241 - accuracy: 0.57 - 6s 269ms/step - loss: 0.7221 - accuracy: 0.5795 - val_loss: 0.6372 - val_accuracy: 0.6161\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.6361 - accuracy: 0.68 - ETA: 3s - loss: 0.6770 - accuracy: 0.59 - ETA: 3s - loss: 0.6816 - accuracy: 0.59 - ETA: 3s - loss: 0.7925 - accuracy: 0.56 - ETA: 2s - loss: 0.8436 - accuracy: 0.54 - ETA: 2s - loss: 0.8138 - accuracy: 0.54 - ETA: 2s - loss: 0.8217 - accuracy: 0.54 - ETA: 2s - loss: 0.8213 - accuracy: 0.54 - ETA: 2s - loss: 0.7961 - accuracy: 0.56 - ETA: 3s - loss: 0.7740 - accuracy: 0.58 - ETA: 3s - loss: 0.7595 - accuracy: 0.58 - ETA: 3s - loss: 0.7632 - accuracy: 0.57 - ETA: 2s - loss: 0.7700 - accuracy: 0.57 - ETA: 2s - loss: 0.7652 - accuracy: 0.57 - ETA: 2s - loss: 0.7667 - accuracy: 0.57 - ETA: 1s - loss: 0.7601 - accuracy: 0.58 - ETA: 1s - loss: 0.7510 - accuracy: 0.58 - ETA: 1s - loss: 0.7423 - accuracy: 0.59 - ETA: 0s - loss: 0.7449 - accuracy: 0.59 - ETA: 0s - loss: 0.7440 - accuracy: 0.58 - ETA: 0s - loss: 0.7462 - accuracy: 0.57 - 6s 269ms/step - loss: 0.7445 - accuracy: 0.5810 - val_loss: 0.6406 - val_accuracy: 0.6116\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.6399 - accuracy: 0.62 - ETA: 3s - loss: 0.6461 - accuracy: 0.62 - ETA: 3s - loss: 0.6910 - accuracy: 0.58 - ETA: 3s - loss: 0.6984 - accuracy: 0.60 - ETA: 2s - loss: 0.6712 - accuracy: 0.63 - ETA: 2s - loss: 0.6908 - accuracy: 0.64 - ETA: 2s - loss: 0.7035 - accuracy: 0.62 - ETA: 2s - loss: 0.7076 - accuracy: 0.62 - ETA: 4s - loss: 0.7054 - accuracy: 0.61 - ETA: 4s - loss: 0.7177 - accuracy: 0.60 - ETA: 3s - loss: 0.7130 - accuracy: 0.60 - ETA: 3s - loss: 0.7140 - accuracy: 0.60 - ETA: 2s - loss: 0.7244 - accuracy: 0.60 - ETA: 2s - loss: 0.7289 - accuracy: 0.59 - ETA: 2s - loss: 0.7225 - accuracy: 0.60 - ETA: 1s - loss: 0.7293 - accuracy: 0.59 - ETA: 1s - loss: 0.7316 - accuracy: 0.59 - ETA: 1s - loss: 0.7338 - accuracy: 0.58 - ETA: 0s - loss: 0.7418 - accuracy: 0.58 - ETA: 0s - loss: 0.7478 - accuracy: 0.58 - ETA: 0s - loss: 0.7553 - accuracy: 0.57 - 6s 270ms/step - loss: 0.7545 - accuracy: 0.5739 - val_loss: 0.6084 - val_accuracy: 0.6116\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.5797 - accuracy: 0.78 - ETA: 3s - loss: 0.6184 - accuracy: 0.70 - ETA: 3s - loss: 0.7091 - accuracy: 0.62 - ETA: 3s - loss: 0.7712 - accuracy: 0.57 - ETA: 3s - loss: 0.7828 - accuracy: 0.58 - ETA: 2s - loss: 0.7540 - accuracy: 0.60 - ETA: 5s - loss: 0.7551 - accuracy: 0.59 - ETA: 6s - loss: 0.7427 - accuracy: 0.59 - ETA: 5s - loss: 0.7358 - accuracy: 0.59 - ETA: 4s - loss: 0.7376 - accuracy: 0.58 - ETA: 4s - loss: 0.7532 - accuracy: 0.57 - ETA: 3s - loss: 0.7662 - accuracy: 0.57 - ETA: 3s - loss: 0.7643 - accuracy: 0.57 - ETA: 2s - loss: 0.7575 - accuracy: 0.58 - ETA: 2s - loss: 0.7478 - accuracy: 0.59 - ETA: 1s - loss: 0.7509 - accuracy: 0.59 - ETA: 1s - loss: 0.7510 - accuracy: 0.59 - ETA: 1s - loss: 0.7500 - accuracy: 0.59 - ETA: 0s - loss: 0.7494 - accuracy: 0.59 - ETA: 0s - loss: 0.7421 - accuracy: 0.60 - ETA: 0s - loss: 0.7351 - accuracy: 0.60 - 6s 275ms/step - loss: 0.7324 - accuracy: 0.6108 - val_loss: 0.5132 - val_accuracy: 0.6562\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7720 - accuracy: 0.56 - ETA: 3s - loss: 0.7509 - accuracy: 0.62 - ETA: 3s - loss: 0.7076 - accuracy: 0.62 - ETA: 3s - loss: 0.7189 - accuracy: 0.60 - ETA: 5s - loss: 0.7183 - accuracy: 0.61 - ETA: 8s - loss: 0.7000 - accuracy: 0.63 - ETA: 7s - loss: 0.7129 - accuracy: 0.62 - ETA: 6s - loss: 0.7144 - accuracy: 0.60 - ETA: 5s - loss: 0.7152 - accuracy: 0.60 - ETA: 4s - loss: 0.7128 - accuracy: 0.60 - ETA: 3s - loss: 0.7194 - accuracy: 0.59 - ETA: 3s - loss: 0.7114 - accuracy: 0.60 - ETA: 2s - loss: 0.7196 - accuracy: 0.59 - ETA: 2s - loss: 0.7222 - accuracy: 0.60 - ETA: 2s - loss: 0.7260 - accuracy: 0.60 - ETA: 1s - loss: 0.7238 - accuracy: 0.60 - ETA: 1s - loss: 0.7227 - accuracy: 0.59 - ETA: 1s - loss: 0.7143 - accuracy: 0.60 - ETA: 0s - loss: 0.7065 - accuracy: 0.61 - ETA: 0s - loss: 0.7092 - accuracy: 0.60 - ETA: 0s - loss: 0.7134 - accuracy: 0.60 - 6s 269ms/step - loss: 0.7175 - accuracy: 0.5994 - val_loss: 0.7029 - val_accuracy: 0.6518\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - ETA: 4s - loss: 0.6253 - accuracy: 0.65 - ETA: 3s - loss: 0.7249 - accuracy: 0.57 - ETA: 3s - loss: 0.6906 - accuracy: 0.60 - ETA: 7s - loss: 0.6958 - accuracy: 0.60 - ETA: 9s - loss: 0.7077 - accuracy: 0.60 - ETA: 8s - loss: 0.7010 - accuracy: 0.60 - ETA: 6s - loss: 0.7201 - accuracy: 0.58 - ETA: 6s - loss: 0.7144 - accuracy: 0.59 - ETA: 5s - loss: 0.7184 - accuracy: 0.59 - ETA: 4s - loss: 0.7200 - accuracy: 0.59 - ETA: 3s - loss: 0.7251 - accuracy: 0.59 - ETA: 3s - loss: 0.7235 - accuracy: 0.59 - ETA: 2s - loss: 0.7216 - accuracy: 0.59 - ETA: 2s - loss: 0.7325 - accuracy: 0.58 - ETA: 2s - loss: 0.7336 - accuracy: 0.58 - ETA: 1s - loss: 0.7247 - accuracy: 0.58 - ETA: 1s - loss: 0.7196 - accuracy: 0.59 - ETA: 1s - loss: 0.7236 - accuracy: 0.58 - ETA: 0s - loss: 0.7312 - accuracy: 0.57 - ETA: 0s - loss: 0.7305 - accuracy: 0.57 - ETA: 0s - loss: 0.7252 - accuracy: 0.58 - 6s 274ms/step - loss: 0.7255 - accuracy: 0.5838 - val_loss: 0.7291 - val_accuracy: 0.6429\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7664 - accuracy: 0.50 - ETA: 12s - loss: 0.6959 - accuracy: 0.546 - ETA: 15s - loss: 0.7226 - accuracy: 0.583 - ETA: 12s - loss: 0.7159 - accuracy: 0.585 - ETA: 9s - loss: 0.7259 - accuracy: 0.575 - ETA: 8s - loss: 0.7769 - accuracy: 0.57 - ETA: 6s - loss: 0.7628 - accuracy: 0.57 - ETA: 6s - loss: 0.7436 - accuracy: 0.59 - ETA: 5s - loss: 0.7368 - accuracy: 0.59 - ETA: 4s - loss: 0.7478 - accuracy: 0.58 - ETA: 3s - loss: 0.7534 - accuracy: 0.56 - ETA: 3s - loss: 0.7492 - accuracy: 0.57 - ETA: 2s - loss: 0.7471 - accuracy: 0.56 - ETA: 2s - loss: 0.7465 - accuracy: 0.56 - ETA: 2s - loss: 0.7430 - accuracy: 0.56 - ETA: 1s - loss: 0.7468 - accuracy: 0.55 - ETA: 1s - loss: 0.7470 - accuracy: 0.56 - ETA: 1s - loss: 0.7414 - accuracy: 0.56 - ETA: 0s - loss: 0.7344 - accuracy: 0.57 - ETA: 0s - loss: 0.7330 - accuracy: 0.56 - ETA: 0s - loss: 0.7313 - accuracy: 0.57 - 8s 362ms/step - loss: 0.7259 - accuracy: 0.5739 - val_loss: 0.6905 - val_accuracy: 0.6562\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - ETA: 6s - loss: 0.8109 - accuracy: 0.53 - ETA: 4s - loss: 0.7668 - accuracy: 0.53 - ETA: 4s - loss: 0.7252 - accuracy: 0.57 - ETA: 3s - loss: 0.6944 - accuracy: 0.60 - ETA: 3s - loss: 0.6964 - accuracy: 0.60 - ETA: 3s - loss: 0.7305 - accuracy: 0.59 - ETA: 3s - loss: 0.7536 - accuracy: 0.58 - ETA: 2s - loss: 0.7721 - accuracy: 0.57 - ETA: 2s - loss: 0.7620 - accuracy: 0.57 - ETA: 2s - loss: 0.7578 - accuracy: 0.57 - ETA: 2s - loss: 0.7668 - accuracy: 0.57 - ETA: 1s - loss: 0.7736 - accuracy: 0.57 - ETA: 1s - loss: 0.7738 - accuracy: 0.56 - ETA: 1s - loss: 0.7697 - accuracy: 0.57 - ETA: 1s - loss: 0.7572 - accuracy: 0.58 - ETA: 1s - loss: 0.7980 - accuracy: 0.58 - ETA: 0s - loss: 0.7861 - accuracy: 0.59 - ETA: 0s - loss: 0.7893 - accuracy: 0.58 - ETA: 0s - loss: 0.7835 - accuracy: 0.58 - ETA: 0s - loss: 0.7805 - accuracy: 0.58 - ETA: 0s - loss: 0.7804 - accuracy: 0.58 - 6s 279ms/step - loss: 0.7802 - accuracy: 0.5781 - val_loss: 0.6610 - val_accuracy: 0.6696\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.5757 - accuracy: 0.71 - ETA: 3s - loss: 0.6373 - accuracy: 0.64 - ETA: 3s - loss: 0.6669 - accuracy: 0.63 - ETA: 3s - loss: 0.6874 - accuracy: 0.61 - ETA: 3s - loss: 0.7390 - accuracy: 0.58 - ETA: 2s - loss: 0.7336 - accuracy: 0.61 - ETA: 2s - loss: 0.7210 - accuracy: 0.60 - ETA: 2s - loss: 0.7341 - accuracy: 0.59 - ETA: 2s - loss: 0.7267 - accuracy: 0.59 - ETA: 2s - loss: 0.7181 - accuracy: 0.59 - ETA: 2s - loss: 0.7094 - accuracy: 0.59 - ETA: 1s - loss: 0.7125 - accuracy: 0.59 - ETA: 1s - loss: 0.7613 - accuracy: 0.58 - ETA: 1s - loss: 0.7590 - accuracy: 0.58 - ETA: 1s - loss: 0.7651 - accuracy: 0.57 - ETA: 1s - loss: 0.7608 - accuracy: 0.57 - ETA: 0s - loss: 0.7615 - accuracy: 0.57 - ETA: 0s - loss: 0.7587 - accuracy: 0.57 - ETA: 0s - loss: 0.7549 - accuracy: 0.58 - ETA: 0s - loss: 0.7496 - accuracy: 0.58 - ETA: 0s - loss: 0.7456 - accuracy: 0.58 - 6s 277ms/step - loss: 0.7469 - accuracy: 0.5810 - val_loss: 0.7186 - val_accuracy: 0.6116\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.5901 - accuracy: 0.78 - ETA: 3s - loss: 0.6628 - accuracy: 0.65 - ETA: 3s - loss: 0.7071 - accuracy: 0.58 - ETA: 3s - loss: 0.6885 - accuracy: 0.60 - ETA: 3s - loss: 0.6956 - accuracy: 0.58 - ETA: 2s - loss: 0.6863 - accuracy: 0.60 - ETA: 2s - loss: 0.6887 - accuracy: 0.61 - ETA: 2s - loss: 0.6974 - accuracy: 0.61 - ETA: 2s - loss: 0.7046 - accuracy: 0.60 - ETA: 2s - loss: 0.7226 - accuracy: 0.60 - ETA: 1s - loss: 0.7161 - accuracy: 0.60 - ETA: 1s - loss: 0.7095 - accuracy: 0.61 - ETA: 1s - loss: 0.7087 - accuracy: 0.61 - ETA: 1s - loss: 0.7081 - accuracy: 0.61 - ETA: 1s - loss: 0.7056 - accuracy: 0.61 - ETA: 1s - loss: 0.7049 - accuracy: 0.61 - ETA: 1s - loss: 0.7046 - accuracy: 0.60 - ETA: 1s - loss: 0.7152 - accuracy: 0.59 - ETA: 0s - loss: 0.7079 - accuracy: 0.60 - ETA: 0s - loss: 0.7106 - accuracy: 0.59 - ETA: 0s - loss: 0.7108 - accuracy: 0.60 - 6s 276ms/step - loss: 0.7088 - accuracy: 0.6037 - val_loss: 0.6876 - val_accuracy: 0.6741\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.6091 - accuracy: 0.78 - ETA: 3s - loss: 0.6608 - accuracy: 0.67 - ETA: 3s - loss: 0.6390 - accuracy: 0.68 - ETA: 3s - loss: 0.6561 - accuracy: 0.67 - ETA: 2s - loss: 0.6339 - accuracy: 0.69 - ETA: 2s - loss: 0.6517 - accuracy: 0.66 - ETA: 2s - loss: 0.6443 - accuracy: 0.66 - ETA: 2s - loss: 0.6565 - accuracy: 0.63 - ETA: 2s - loss: 0.6520 - accuracy: 0.64 - ETA: 2s - loss: 0.6630 - accuracy: 0.62 - ETA: 1s - loss: 0.6713 - accuracy: 0.61 - ETA: 1s - loss: 0.6713 - accuracy: 0.61 - ETA: 1s - loss: 0.6756 - accuracy: 0.61 - ETA: 1s - loss: 0.6714 - accuracy: 0.62 - ETA: 1s - loss: 0.6905 - accuracy: 0.60 - ETA: 1s - loss: 0.6842 - accuracy: 0.60 - ETA: 1s - loss: 0.6879 - accuracy: 0.61 - ETA: 1s - loss: 0.6919 - accuracy: 0.60 - ETA: 0s - loss: 0.6877 - accuracy: 0.60 - ETA: 0s - loss: 0.6926 - accuracy: 0.59 - ETA: 0s - loss: 0.6974 - accuracy: 0.60 - 6s 275ms/step - loss: 0.6960 - accuracy: 0.6009 - val_loss: 0.6550 - val_accuracy: 0.5580\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.8926 - accuracy: 0.62 - ETA: 3s - loss: 0.6889 - accuracy: 0.71 - ETA: 3s - loss: 0.7071 - accuracy: 0.69 - ETA: 3s - loss: 0.7052 - accuracy: 0.65 - ETA: 2s - loss: 0.6988 - accuracy: 0.65 - ETA: 2s - loss: 0.6923 - accuracy: 0.65 - ETA: 2s - loss: 0.7118 - accuracy: 0.63 - ETA: 2s - loss: 0.7206 - accuracy: 0.62 - ETA: 2s - loss: 0.7147 - accuracy: 0.62 - ETA: 2s - loss: 0.7241 - accuracy: 0.60 - ETA: 1s - loss: 0.7279 - accuracy: 0.59 - ETA: 2s - loss: 0.7245 - accuracy: 0.59 - ETA: 2s - loss: 0.7223 - accuracy: 0.59 - ETA: 2s - loss: 0.7211 - accuracy: 0.59 - ETA: 2s - loss: 0.7275 - accuracy: 0.59 - ETA: 1s - loss: 0.7252 - accuracy: 0.59 - ETA: 1s - loss: 0.7241 - accuracy: 0.59 - ETA: 1s - loss: 0.7286 - accuracy: 0.58 - ETA: 0s - loss: 0.7322 - accuracy: 0.58 - ETA: 0s - loss: 0.7325 - accuracy: 0.58 - ETA: 0s - loss: 0.7307 - accuracy: 0.58 - 6s 276ms/step - loss: 0.7264 - accuracy: 0.5881 - val_loss: 0.7793 - val_accuracy: 0.6205\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.6830 - accuracy: 0.56 - ETA: 3s - loss: 0.6806 - accuracy: 0.62 - ETA: 3s - loss: 0.7334 - accuracy: 0.62 - ETA: 3s - loss: 0.7162 - accuracy: 0.61 - ETA: 2s - loss: 0.6891 - accuracy: 0.62 - ETA: 2s - loss: 0.6944 - accuracy: 0.60 - ETA: 2s - loss: 0.7002 - accuracy: 0.60 - ETA: 2s - loss: 0.7009 - accuracy: 0.61 - ETA: 2s - loss: 0.7186 - accuracy: 0.59 - ETA: 2s - loss: 0.7115 - accuracy: 0.58 - ETA: 3s - loss: 0.7063 - accuracy: 0.58 - ETA: 3s - loss: 0.7081 - accuracy: 0.58 - ETA: 2s - loss: 0.7101 - accuracy: 0.58 - ETA: 2s - loss: 0.7044 - accuracy: 0.58 - ETA: 2s - loss: 0.7111 - accuracy: 0.57 - ETA: 1s - loss: 0.7195 - accuracy: 0.57 - ETA: 1s - loss: 0.7135 - accuracy: 0.58 - ETA: 1s - loss: 0.7203 - accuracy: 0.57 - ETA: 0s - loss: 0.7217 - accuracy: 0.57 - ETA: 0s - loss: 0.7177 - accuracy: 0.57 - ETA: 0s - loss: 0.7105 - accuracy: 0.58 - 6s 274ms/step - loss: 0.7055 - accuracy: 0.5852 - val_loss: 0.6077 - val_accuracy: 0.5938\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - ETA: 4s - loss: 0.6591 - accuracy: 0.56 - ETA: 3s - loss: 0.6402 - accuracy: 0.64 - ETA: 3s - loss: 0.6128 - accuracy: 0.66 - ETA: 3s - loss: 0.6420 - accuracy: 0.63 - ETA: 3s - loss: 0.6731 - accuracy: 0.61 - ETA: 2s - loss: 0.6782 - accuracy: 0.59 - ETA: 2s - loss: 0.6860 - accuracy: 0.59 - ETA: 3s - loss: 0.6816 - accuracy: 0.58 - ETA: 5s - loss: 0.7052 - accuracy: 0.59 - ETA: 4s - loss: 0.7050 - accuracy: 0.58 - ETA: 4s - loss: 0.7060 - accuracy: 0.57 - ETA: 3s - loss: 0.7078 - accuracy: 0.58 - ETA: 3s - loss: 0.7026 - accuracy: 0.58 - ETA: 2s - loss: 0.7019 - accuracy: 0.57 - ETA: 2s - loss: 0.7092 - accuracy: 0.57 - ETA: 1s - loss: 0.7092 - accuracy: 0.56 - ETA: 1s - loss: 0.7040 - accuracy: 0.57 - ETA: 1s - loss: 0.6989 - accuracy: 0.57 - ETA: 0s - loss: 0.7000 - accuracy: 0.57 - ETA: 0s - loss: 0.6992 - accuracy: 0.57 - ETA: 0s - loss: 0.7040 - accuracy: 0.57 - 6s 278ms/step - loss: 0.7010 - accuracy: 0.5810 - val_loss: 0.5915 - val_accuracy: 0.6384\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - ETA: 4s - loss: 0.6855 - accuracy: 0.59 - ETA: 3s - loss: 0.6609 - accuracy: 0.60 - ETA: 3s - loss: 0.6668 - accuracy: 0.60 - ETA: 3s - loss: 0.6962 - accuracy: 0.59 - ETA: 3s - loss: 0.7008 - accuracy: 0.60 - ETA: 5s - loss: 0.7471 - accuracy: 0.58 - ETA: 7s - loss: 0.7804 - accuracy: 0.58 - ETA: 6s - loss: 0.7866 - accuracy: 0.57 - ETA: 5s - loss: 0.7683 - accuracy: 0.58 - ETA: 4s - loss: 0.7731 - accuracy: 0.58 - ETA: 4s - loss: 0.7598 - accuracy: 0.58 - ETA: 3s - loss: 0.7632 - accuracy: 0.57 - ETA: 3s - loss: 0.7635 - accuracy: 0.57 - ETA: 2s - loss: 0.7710 - accuracy: 0.56 - ETA: 2s - loss: 0.7652 - accuracy: 0.56 - ETA: 1s - loss: 0.7725 - accuracy: 0.55 - ETA: 1s - loss: 0.7640 - accuracy: 0.56 - ETA: 1s - loss: 0.7636 - accuracy: 0.56 - ETA: 0s - loss: 0.7582 - accuracy: 0.56 - ETA: 0s - loss: 0.7573 - accuracy: 0.56 - ETA: 0s - loss: 0.7603 - accuracy: 0.56 - 6s 279ms/step - loss: 0.7549 - accuracy: 0.5682 - val_loss: 0.7496 - val_accuracy: 0.6250\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - ETA: 3s - loss: 0.7103 - accuracy: 0.53 - ETA: 3s - loss: 0.7183 - accuracy: 0.60 - ETA: 6s - loss: 0.7143 - accuracy: 0.60 - ETA: 9s - loss: 0.7348 - accuracy: 0.58 - ETA: 9s - loss: 0.7454 - accuracy: 0.59 - ETA: 8s - loss: 0.7363 - accuracy: 0.59 - ETA: 7s - loss: 0.7324 - accuracy: 0.59 - ETA: 6s - loss: 0.7249 - accuracy: 0.59 - ETA: 5s - loss: 0.7121 - accuracy: 0.60 - ETA: 4s - loss: 0.7159 - accuracy: 0.61 - ETA: 4s - loss: 0.7154 - accuracy: 0.61 - ETA: 3s - loss: 0.7131 - accuracy: 0.60 - ETA: 3s - loss: 0.7062 - accuracy: 0.61 - ETA: 2s - loss: 0.6986 - accuracy: 0.62 - ETA: 2s - loss: 0.7004 - accuracy: 0.62 - ETA: 1s - loss: 0.6919 - accuracy: 0.62 - ETA: 1s - loss: 0.6964 - accuracy: 0.62 - ETA: 1s - loss: 0.6996 - accuracy: 0.61 - ETA: 0s - loss: 0.7056 - accuracy: 0.60 - ETA: 0s - loss: 0.7029 - accuracy: 0.61 - ETA: 0s - loss: 0.6952 - accuracy: 0.61 - 6s 276ms/step - loss: 0.7015 - accuracy: 0.6151 - val_loss: 0.6793 - val_accuracy: 0.6116\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(gentrain, \n",
    "                     steps_per_epoch=TRAIN_SIZE//BATCH_SIZE,\n",
    "                     verbose=1,\n",
    "                     validation_data=gentest,\n",
    "                     validation_steps=TEST_SIZE//BATCH_SIZE,\n",
    "                     epochs=NUM_EPOCHS,\n",
    "                     workers=3,\n",
    "                     use_multiprocessing=False)\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6777810049057007, 0.6050000190734863]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=x_test,y=y_test,\n",
    "              batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}